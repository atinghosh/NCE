{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data2/atin/anaconda3/envs/pytorch/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/data2/atin/anaconda3/envs/pytorch/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/data2/atin/anaconda3/envs/pytorch/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/data2/atin/anaconda3/envs/pytorch/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/data2/atin/anaconda3/envs/pytorch/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/data2/atin/anaconda3/envs/pytorch/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Loading faiss with AVX2 support.\n"
     ]
    }
   ],
   "source": [
    "#Most of the code is copied from https://github.com/mangye16/Unsupervised_Embedding_Learning\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "import math\n",
    "from easydict import EasyDict as edict\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import umap\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "from matplotlib.figure import Figure\n",
    "\n",
    "from scipy.sparse.linalg import cg\n",
    "from scipy.sparse import csr_matrix, identity, diags\n",
    "\n",
    "import faiss\n",
    "from progressbar import ProgressBar\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "# Hard location of path \n",
    "# Labels = \"../labels/cifar10/250_balanced_labels/00.txt\" \n",
    "suffix = 'cifar_fixed_weight_label_unlabel_separate_factor_schedule_all_data_ramp_up_1' # Change this suffix to a suitable name for the experiment for tensorboard and logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "def generate_subset_of_CIFAR_for_ssl(samples_per_class, label_sample_per_class=25, seed=1):\n",
    "    '''Generate label and unalabel index for CIFAR10, total unlabel index is same as no of samples\n",
    "    \n",
    "    Args:\n",
    "        \n",
    "        samples_per_class(int): no. of images to be considered per class, maximum posisble is 5000\n",
    "        \n",
    "        label_sample_per_class(int): no. of label images per class, must be less than equal to samples_per_class, \n",
    "        default is 25\n",
    "        \n",
    "    \n",
    "    Returns:\n",
    "        index for unlabel and label\n",
    "    '''\n",
    " \n",
    "    trainset_cifar = datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "    trainloader = torch.utils.data.DataLoader(trainset_cifar, batch_size=1024, shuffle=False, num_workers=20)\n",
    "    \n",
    "    list_of_target = []\n",
    "    for i, (_, target) in enumerate(trainloader):\n",
    "        list_of_target.append(target)\n",
    "    list_of_target = torch.cat(list_of_target).numpy()\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    sampled_index = []\n",
    "    index_for_label = []\n",
    "    for i in range(10):\n",
    "        ii = np.where(list_of_target == i)[0]\n",
    "        cls_sample = list(np.random.choice(ii, samples_per_class, replace=False))\n",
    "        sampled_index.extend(cls_sample)\n",
    "        cls_sample_labeled = list(np.random.choice(cls_sample, label_sample_per_class, replace=False))\n",
    "        index_for_label.extend(cls_sample_labeled)\n",
    "    \n",
    "    sampled_index = list(set(sampled_index)-set(index_for_label))\n",
    "    return sampled_index, index_for_label\n",
    "\n",
    "unlabel_index, label_index = generate_subset_of_CIFAR_for_ssl(5000, 25, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch-t: softmax temperature parameter (0.05-0.1)\n",
    "# low-dim: the feature embedding dimension (default: 128)\n",
    "\n",
    "args = edict({'dataset':'cifar', 'lr': .1, 'resume': '', 'log_dir': 'log/', 'model_dir': 'checkpoint/',\n",
    "              'test_epoch': 1, 'low_dim': 128, 'batch_t': .1, 'batch_m': 1, 'batch_size': 500, 'gpu': '0, 1, 2',\n",
    "              'alpha': .8 }) \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "vis_log_dir = args.log_dir + suffix + '/'\n",
    "if not os.path.isdir(vis_log_dir):\n",
    "    os.makedirs(vis_log_dir)\n",
    "writer = SummaryWriter(log_dir=vis_log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resnet Model\n",
    "\n",
    "class Normalize(nn.Module):\n",
    "\n",
    "    def __init__(self, power=2, temp=1):\n",
    "        super(Normalize, self).__init__()\n",
    "        self.power = power\n",
    "        self.temp = temp\n",
    "    \n",
    "    def forward(self, x):\n",
    "        norm = x.pow(self.power).sum(1, keepdim=True).pow(1./self.power)\n",
    "        out = x.div(norm)\n",
    "        return out/self.temp\n",
    "    \n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, pool_len =4, low_dim=128, fixed_classifier=False, temp=1):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.temp = temp\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear_embedding = nn.Linear(512*block.expansion, low_dim)\n",
    "        self.linear_class = nn.Linear(low_dim, 10, bias=False)\n",
    "        if fixed_classifier:\n",
    "            M = np.random.normal(0, 1, size=(low_dim, low_dim))\n",
    "            ortho, _, _ = np.linalg.svd(M)\n",
    "            ortho = ortho[:10]\n",
    "            self.linear_class.weight.data = torch.tensor(ortho, dtype=torch.float).cuda()\n",
    "            self.linear_class.weight.detach_()\n",
    "        # with torch.no_grad():\n",
    "        #     self.linear_class.weight.div_(torch.norm(self.linear_class.weight, dim=1, keepdim=True))\n",
    "        self.l2norm_for_feature = Normalize()\n",
    "        # self.l2norm_for_weight = Normalize(temp=.1)\n",
    "        self.pool_len = pool_len\n",
    "        # w = torch.nn.Parameter(torch.randn(128, 10))\n",
    "        # for m in self.modules():\n",
    "            # if isinstance(m, nn.Conv2d):\n",
    "                # n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                # m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            # elif isinstance(m, nn.BatchNorm2d):\n",
    "                # m.weight.data.fill_(1)\n",
    "                # m.bias.data.zero_()\n",
    "                \n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, self.pool_len)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out_embedding = self.linear_embedding(out)\n",
    "        out_embedding = self.l2norm_for_feature(out_embedding)\n",
    "        # with torch.no_grad():\n",
    "        #     self.linear_class.weight.div_(torch.norm(self.linear_class.weight, dim=1, keepdim=True))\n",
    "        # self.linear_class.weight.div_(torch.norm(self.linear_class.weight, dim=1, keepdim=True))\n",
    "        # type(self.l2norm_for_weight(self.linear_class.weight))\n",
    "        # self.linear_class.weight = self.l2norm_for_weight(self.linear_class.weight)\n",
    "        # self.linear_class.weight = torch.nn.Parameter(10 * self.linear_class.weight/torch.norm(self.linear_class.weight, dim=1, keepdim=True))\n",
    "        # out_class = 10*self.linear_class(out_embedding)/torch.norm(self.linear_class.weight, dim=1, keepdim=True).transpose(1,0)\n",
    "        out_class = self.linear_class(out_embedding)\n",
    "        return out_embedding, out_class/self.temp\n",
    "        # return out_embedding, out_class\n",
    "\n",
    "    # def normalize_weight(self):\n",
    "    #     with torch.no_grad():\n",
    "    #         self.linear_class.weight.div_(torch.norm(self.linear_class.weight, dim=1, keepdim=True))\n",
    "\n",
    "\n",
    "\n",
    "def ResNet18(pool_len = 4, low_dim=128, fixed_weight=True, temperature=1):\n",
    "    return ResNet(BasicBlock, [2,2,2,2], pool_len, low_dim, fixed_classifier= fixed_weight, temp=temperature)\n",
    "\n",
    "\n",
    "class BatchCriterion(nn.Module):  # Unsupervised Loss\n",
    "    ''' Compute the unsupervised loss within each batch  \n",
    "    '''\n",
    "    def __init__(self, T, batchSize):\n",
    "        super(BatchCriterion, self).__init__()\n",
    "        self.T = T\n",
    "        self.diag_mat = 1 - torch.eye(batchSize*2).cuda()\n",
    "        \n",
    "    def forward(self, x, targets):\n",
    "        batchSize = x.size(0)\n",
    "        \n",
    "        #get positive innerproduct\n",
    "        reordered_x = torch.cat((x.narrow(0,batchSize//2,batchSize//2), x.narrow(0,0,batchSize//2)), 0)\n",
    "        #reordered_x = reordered_x.data\n",
    "        pos = (x*reordered_x.data).sum(1).div_(self.T).exp_()\n",
    "\n",
    "        #get all innerproduct, remove diag\n",
    "        all_prob = torch.mm(x,x.t().data).div_(self.T).exp_()*self.diag_mat\n",
    "        all_div = all_prob.sum(1)\n",
    "        \n",
    "\n",
    "        lnPmt = torch.div(pos, all_div)\n",
    "\n",
    "        # negative probability\n",
    "        Pon_div = all_div.repeat(batchSize,1)\n",
    "        lnPon = torch.div(all_prob, Pon_div.t())\n",
    "        lnPon = -lnPon.add(-1)\n",
    "        \n",
    "        # equation 7 in ref. A (NCE paper)\n",
    "        lnPon.log_()\n",
    "        # also remove the pos term\n",
    "        lnPon = lnPon.sum(1) - (-lnPmt.add(-1)).log_()\n",
    "        lnPmt.log_()\n",
    "\n",
    "        lnPmtsum = lnPmt.sum(0)\n",
    "        lnPonsum = lnPon.sum(0)\n",
    "\n",
    "        loss = - (lnPmtsum + lnPonsum)/batchSize\n",
    "        return loss\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed at 120, 160 and 200\"\"\"\n",
    "    lr = args.lr\n",
    "    if epoch >= 120 and epoch < 160:\n",
    "        lr = args.lr * 0.1\n",
    "    elif epoch >= 160 and epoch < 200:\n",
    "        lr = args.lr * 0.05\n",
    "    elif epoch >= 200:\n",
    "        lr = args.lr * 0.01\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    writer.add_scalar('lr', lr, epoch)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10Instance(datasets.CIFAR10):\n",
    "    \"\"\"CIFAR10Instance Dataset.\n",
    "    \"\"\"\n",
    "    def __getitem__(self, index):\n",
    "        if self.train:\n",
    "            img, target = self.data[index], self.targets[index]\n",
    "        else:\n",
    "            img, target = self.data[index], self.targets[index]\n",
    "\n",
    "        # doing this so that it is consistent with all other datasets\n",
    "        # to return a PIL Image\n",
    "        img = Image.fromarray(img)\n",
    "        \n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "            \n",
    "        if self.transform is not None:\n",
    "            img1 = self.transform(img)\n",
    "            if self.train:\n",
    "                img2 = self.transform(img)\n",
    "\n",
    "        if self.train:\n",
    "            return img1, img2, target, index\n",
    "        else:\n",
    "            return img1, target, index\n",
    "\n",
    "\n",
    "class Cutout(object):\n",
    "    \"\"\"Randomly mask out one or more patches from an image.\n",
    "    Args:\n",
    "        n_holes (int): Number of patches to cut out of each image.\n",
    "        length (int): The length (in pixels) of each square patch.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_holes, length):\n",
    "        self.n_holes = n_holes\n",
    "        self.length = length\n",
    "\n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (Tensor): Tensor image of size (C, H, W).\n",
    "        Returns:\n",
    "            Tensor: Image with n_holes of dimension length x length cut out of it.\n",
    "        \"\"\"\n",
    "        h = img.size(1)\n",
    "        w = img.size(2)\n",
    "\n",
    "        mask = np.ones((h, w), np.float32)\n",
    "\n",
    "        for n in range(self.n_holes):\n",
    "            y = np.random.randint(h)\n",
    "            x = np.random.randint(w)\n",
    "\n",
    "            y1 = np.clip(y - self.length // 2, 0, h)\n",
    "            y2 = np.clip(y + self.length // 2, 0, h)\n",
    "            x1 = np.clip(x - self.length // 2, 0, w)\n",
    "            x2 = np.clip(x + self.length // 2, 0, w)\n",
    "\n",
    "            mask[y1: y2, x1: x2] = 0.\n",
    "\n",
    "        mask = torch.from_numpy(mask)\n",
    "        mask = mask.expand_as(img)\n",
    "        img = img * mask\n",
    "\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\" \n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "                   \n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0 \n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def kNN(epoch, net, trainloader, testloader, K, sigma, ndata, low_dim=128):\n",
    "    net.eval()\n",
    "    net_time = AverageMeter()\n",
    "    cls_time = AverageMeter()\n",
    "    total = 0\n",
    "    correct_t = 0\n",
    "    testsize = testloader.dataset.__len__()\n",
    "\n",
    "    if hasattr(trainloader.dataset, 'imgs'):\n",
    "        trainLabels = torch.LongTensor([y for (p, y) in trainloader.dataset.imgs]).cuda()\n",
    "    else:\n",
    "        try:\n",
    "            trainLabels = torch.LongTensor(trainloader.dataset.train_labels).cuda()\n",
    "        except:\n",
    "            trainLabels = torch.LongTensor(trainloader.dataset.targets).cuda()\n",
    "    trainFeatures = np.zeros((low_dim, ndata))\n",
    "    C = trainLabels.max() + 1\n",
    "    C = np.int(C)\n",
    "    with torch.no_grad():\n",
    "        transform_bak = trainloader.dataset.transform\n",
    "        trainloader.dataset.transform = testloader.dataset.transform\n",
    "        temploader = torch.utils.data.DataLoader(trainloader.dataset, batch_size=100, shuffle=False, num_workers=10)\n",
    "        for batch_idx, (inputs, _, targets, indexes) in enumerate(temploader):\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            batchSize = inputs.size(0)\n",
    "            features, _ = net(inputs)\n",
    "            #\n",
    "            trainFeatures[:, batch_idx * batchSize:batch_idx * batchSize + batchSize] = features.data.t().cpu().numpy()\n",
    "\n",
    "    trainloader.dataset.transform = transform_bak\n",
    "    #\n",
    "\n",
    "    trainFeatures = torch.Tensor(trainFeatures).cuda()\n",
    "    top1 = 0.\n",
    "    top5 = 0.\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        retrieval_one_hot = torch.zeros(K, C).cuda()\n",
    "        for batch_idx, (inputs, targets, indexes) in enumerate(testloader):\n",
    "            end = time.time()\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            batchSize = inputs.size(0)\n",
    "            features, _ = net(inputs)\n",
    "            total += targets.size(0)\n",
    "\n",
    "            net_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            dist = torch.mm(features, trainFeatures)\n",
    "            yd, yi = dist.topk(K, dim=1, largest=True, sorted=True)\n",
    "            candidates = trainLabels.view(1, -1).expand(batchSize, -1)\n",
    "            retrieval = torch.gather(candidates, 1, yi)\n",
    "\n",
    "            retrieval_one_hot.resize_(batchSize * K, C).zero_()\n",
    "            retrieval_one_hot.scatter_(1, retrieval.view(-1, 1), 1)\n",
    "            yd_transform = yd.clone().div_(sigma).exp_()\n",
    "            probs = torch.sum(torch.mul(retrieval_one_hot.view(batchSize, -1, C), yd_transform.view(batchSize, -1, 1)),\n",
    "                              1)\n",
    "            _, predictions = probs.sort(1, True)\n",
    "\n",
    "            # Find which predictions match the target\n",
    "            correct = predictions.eq(targets.data.view(-1, 1))\n",
    "            cls_time.update(time.time() - end)\n",
    "\n",
    "            top1 = top1 + correct.narrow(1, 0, 1).sum().item()\n",
    "            top5 = top5 + correct.narrow(1, 0, 5).sum().item()\n",
    "\n",
    "            print('Test [{}/{}]\\t'\n",
    "                  'Net Time {net_time.val:.3f} ({net_time.avg:.3f})\\t'\n",
    "                  'Cls Time {cls_time.val:.3f} ({cls_time.avg:.3f})\\t'\n",
    "                  'Top1: {:.2f}  Top5: {:.2f}'.format(\n",
    "                total, testsize, top1 * 100. / total, top5 * 100. / total, net_time=net_time, cls_time=cls_time))\n",
    "\n",
    "    print(top1 * 100. / total)\n",
    "\n",
    "    return top1 * 100. / total\n",
    "\n",
    "def return_train_labels_index(Labels_loc):\n",
    "    label_list = []\n",
    "    with open(Labels_loc, 'r') as df:\n",
    "        for l in df.readlines():\n",
    "            a = l.rstrip().split(\"_\")\n",
    "            label_list.append(int(a[0]))\n",
    "\n",
    "    return np.array(label_list)\n",
    "\n",
    "def linear_rampup(current, rampup_length):\n",
    "    \"\"\"Linear rampup\"\"\"\n",
    "    assert current >= 0 and rampup_length >= 0\n",
    "    if current >= rampup_length:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return current / rampup_length\n",
    "    \n",
    "def sem_sup_feature(conv_model: nn.Module, dl: torch.utils.data.dataloader):\n",
    "    '''Extracts features from images using pretrained model\n",
    "\n",
    "    Args:\n",
    "        dl: dataloader\n",
    "        conv_model: pretrained nn.Module object\n",
    "\n",
    "    Returns:\n",
    "        Tuple[list1, list2]\n",
    "        list1: 2D torch tensor of size (no. of observation, embedding dim)\n",
    "        list2: 1D array of corresponding labels\n",
    "    '''\n",
    "    conv_model.eval()\n",
    "    semi_supervised_feature_list = []\n",
    "    label_list = []\n",
    "    for b in dl:\n",
    "        data, label = b\n",
    "        data = data.cuda()\n",
    "        with torch.no_grad():\n",
    "            out, _ = conv_model(data)\n",
    "            b_size = out\n",
    "            semi_supervised_feature_list.append(out)\n",
    "            label_list.append(label)\n",
    "    final_list = torch.cat(semi_supervised_feature_list, dim=0)\n",
    "    final_label_list = torch.cat(label_list, dim=0)\n",
    "    return final_list.cpu().numpy(), final_label_list.cpu().numpy()\n",
    "\n",
    "#create data loaders\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                           download=True, transform=transform_test)\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                            download=True, transform=transform_test)\n",
    "\n",
    "combined_dataset = torch.utils.data.ConcatDataset([trainset, testset])\n",
    "combined_dataloader = torch.utils.data.DataLoader(combined_dataset, batch_size=1024, shuffle=False, num_workers=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildGraph(MatX, trns, knn_num_neighbors=20, dim=128) -> csr_matrix:\n",
    "    '''creates the affinity matrix from the fearure matrix\n",
    "    \n",
    "    MatX has a particular structure. \n",
    "    \n",
    "    Args:\n",
    "        Matx: feature matrix of shape (num_observation, embedding_dimension). It must have the following structure.\n",
    "        The first 250 rows (assuming our SSL is trained on 250 labels) will have features corresponding to 250\n",
    "        labelled examples and remaining rows will be features for unlabelled examples.\n",
    "        \n",
    "        trns(function object): transformation for every element of affinity matrix e.g. lambda x: 0 if x < 0 else x**4\n",
    "        knn_num_neighbors: # of nearest neighbors\n",
    "        \n",
    "        dim: embedding dimension\n",
    "    \n",
    "    Returns:\n",
    "        sparse affinity matrix to be used for label propagation later, where labelled examples are stacked at the\n",
    "        front rows. This is required for label propagation function to work properly. \n",
    "    '''\n",
    "    \n",
    "    num_samples = MatX.shape[0]\n",
    "    index = faiss.IndexFlatIP(dim)\n",
    "    index.add(MatX)\n",
    "    distances, indices = index.search(MatX, knn_num_neighbors)\n",
    "\n",
    "    trns = np.vectorize(trns)\n",
    "\n",
    "    row = np.repeat(np.arange(num_samples), knn_num_neighbors)\n",
    "    col = indices.flatten()\n",
    "    data = distances.flatten()\n",
    "    data = trns(data)\n",
    "    sp_affinity_matrix = csr_matrix((data, (row, col)), shape=(num_samples, num_samples))\n",
    "    sp_affinity_matrix = (sp_affinity_matrix + sp_affinity_matrix.transpose())/2\n",
    "    return sp_affinity_matrix\n",
    "\n",
    "\n",
    "def labelPropagation(sp_affinity, Mat_Label, Mat_Unlabel, labels, alpha=.1, n_iter=100) -> np.array:\n",
    "    '''Propagates the label to get the prefiction for all unlabelled observations\n",
    "    \n",
    "    Args:\n",
    "        sp_affinity: Sparse affinity matrix of shape (num_observation, num_observation).It must have the following structure.\n",
    "        The first 250 rows (assuming our SSL is trained on 250 labels) will be corresponding to 250\n",
    "        labelled examples and remaining rows will be corresponding to unlabelled examples.\n",
    "        \n",
    "        Mat_Label: Feature matrix corresponding to lablled obs.\n",
    "        \n",
    "        Mat_Unlabel: Feature matrix corresponding to unlablled obs.\n",
    "        \n",
    "        labels: labels for each row in Mat_Label\n",
    "    \n",
    "    \n",
    "    Returns:\n",
    "        predicted labels for all rows of Mat_Unlabel\n",
    "    '''\n",
    "    \n",
    "    # initialize\n",
    "    num_label_samples = Mat_Label.shape[0]\n",
    "    num_unlabel_samples = Mat_Unlabel.shape[0]\n",
    "    num_samples = num_label_samples + num_unlabel_samples\n",
    "    labels_list = np.unique(labels)\n",
    "    num_classes = len(labels_list)\n",
    "\n",
    "    clamp_data_label = np.zeros((num_label_samples, num_classes), np.float32)\n",
    "    for i in range(num_label_samples):\n",
    "        clamp_data_label[i][labels[i]] = 1.0\n",
    "\n",
    "    label_function = np.zeros((num_samples, num_classes), np.float32)\n",
    "    label_function[0: num_label_samples] = clamp_data_label\n",
    "    label_function[num_label_samples: num_samples] = 0\n",
    "    \n",
    "    degree_vec = np.sum(sp_affinity, axis=1)\n",
    "    degree_vec_to_the_power_minus_half = 1/np.sqrt(degree_vec)\n",
    "    sp_degree_matrix_2_the_power_minus_half = diags(np.array(degree_vec_to_the_power_minus_half).flatten())\n",
    "\n",
    "    sp_d_minus_half_w_d_minus_half = sp_degree_matrix_2_the_power_minus_half @ sp_affinity @ sp_degree_matrix_2_the_power_minus_half\n",
    "\n",
    "    sparse_matrix = identity(num_samples, format=\"csr\") - alpha * sp_d_minus_half_w_d_minus_half\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    normalization_diag = diags(np.array(1. / degree_vec).flatten())\n",
    "    P = normalization_diag @ sp_affinity\n",
    "    label_function_prop = np.copy(label_function)\n",
    "    for k in range(n_iter):\n",
    "        label_function_prop = P @ label_function_prop\n",
    "        label_function_prop[:num_label_samples] = clamp_data_label\n",
    "        unlabel_data_labels = np.argmax(label_function_prop, axis=1)\n",
    "        unlabel_data_labels = unlabel_data_labels[num_label_samples:]\n",
    "\n",
    "\n",
    "    return unlabel_data_labels\n",
    "\n",
    "def get_acc(predicted_labels, true_labels):\n",
    "    '''returns accuracy'''\n",
    "    \n",
    "    corrects = 0\n",
    "    num_samples = len(predicted_labels)\n",
    "    for i in range(num_samples):\n",
    "        if predicted_labels[i] == true_labels[i]:\n",
    "            corrects +=1\n",
    "    return corrects/num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "==> Building model..\n"
     ]
    }
   ],
   "source": [
    "print('==> Preparing data..')\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(size=32, scale=(0.2, 1.)),\n",
    "    transforms.ColorJitter(0.4, 0.4, 0.4, 0.4),\n",
    "    transforms.RandomGrayscale(p=0.2),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    Cutout(1, 8),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# labeled dataloader\n",
    "custom_sampler_label = SubsetRandomSampler(label_index)\n",
    "trainset_labeled = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "labeled_dataloader = torch.utils.data.DataLoader(trainset_labeled, batch_size=50, sampler=custom_sampler_label, \n",
    "                                                 num_workers=0)\n",
    "labeled_dataloader_iter = iter(labeled_dataloader)\n",
    "\n",
    "\n",
    "# Unlabeled dataloader\n",
    "custom_sampler_unlabel = SubsetRandomSampler(unlabel_index)\n",
    "\n",
    "\n",
    "trainset = CIFAR10Instance(root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=args.batch_size, sampler=custom_sampler_unlabel, \n",
    "                                          num_workers=20, drop_last=True)\n",
    "testset = CIFAR10Instance(root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=10)\n",
    "\n",
    "\n",
    "print('==> Building model..')\n",
    "net = ResNet18(pool_len=4, low_dim=args.low_dim, fixed_weight=True, temperature=.1)\n",
    "\n",
    "# to use attributes from dataparallel \n",
    "class MyDataParallel(nn.DataParallel):\n",
    "    def __getattr__(self, name):\n",
    "        try:\n",
    "            return super().__getattr__(name)\n",
    "        except AttributeError:\n",
    "            return getattr(self.module, name)\n",
    "\n",
    "if device == 'cuda':\n",
    "    # net = torch.nn.DataParallel(net, device_ids=range(torch.cuda.device_count()))\n",
    "    net = MyDataParallel(net, device_ids=range(torch.cuda.device_count()))\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "\n",
    "# define loss function: inner product loss within each mini-batch\n",
    "criterion = BatchCriterion(args.batch_t, args.batch_size)\n",
    "supervised_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "net.to(device)\n",
    "criterion.to(device)\n",
    "supervised_criterion.to(device)\n",
    "\n",
    "# define optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=args.lr, momentum=0.9, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_finder(nb_epoch, start=-3, end=.1):\n",
    "    \n",
    "    global trainloader, labeled_dataloader, labeled_dataloader_iter\n",
    "    lr_list = np.power(np.repeat(10, len(trainloader)*nb_epoch), np.linspace(start, end, len(trainloader)*nb_epoch))\n",
    "    \n",
    "    net = ResNet18(pool_len=4, low_dim=args.low_dim)\n",
    "    if device == 'cuda':\n",
    "        net = MyDataParallel(net, device_ids=range(torch.cuda.device_count()))\n",
    "        cudnn.benchmark = True\n",
    "    net.to(device)\n",
    "    optimizer = optim.SGD(net.parameters(), lr=args.lr, momentum=0.9, weight_decay=5e-4)\n",
    "    \n",
    "    def adjust_lr_for_lr_finder(optimizer, lr_value):\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr_value\n",
    "\n",
    "    \n",
    "    \n",
    "    current_ind = 0 \n",
    "    def train():\n",
    "        nonlocal current_ind\n",
    "        \n",
    "        train_loss = AverageMeter()\n",
    "        supervised_loss = AverageMeter()\n",
    "        \n",
    "\n",
    "        # switch to train mode\n",
    "        net.train()\n",
    "        \n",
    "        sup_loss_list = []\n",
    "        train_loss_list = []\n",
    "        for batch_idx, (inputs1, inputs2, _, indexes) in enumerate(trainloader):\n",
    "            try:\n",
    "                labeled_input, target = labeled_dataloader_iter.next()\n",
    "            except:\n",
    "                labeled_dataloader_iter = iter(labeled_dataloader)\n",
    "                labeled_input, target = labeled_dataloader_iter.next()\n",
    "\n",
    "            label_batchsize = labeled_input.size(0)\n",
    "            inputs = torch.cat((labeled_input, inputs1, inputs2), 0)\n",
    "            inputs, target, indexes = inputs.to(device), target.to(device), indexes.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            adjust_lr_for_lr_finder(optimizer, lr_list[current_ind])\n",
    "            print(lr_list[current_ind])\n",
    "            current_ind = current_ind + 1\n",
    "\n",
    "            features, pred = net(inputs)\n",
    "            features = features[label_batchsize:]\n",
    "            pred = pred[:label_batchsize]\n",
    "\n",
    "            loss = criterion(features, indexes)\n",
    "            sup_loss = supervised_criterion(pred, target)\n",
    "\n",
    "            factor = 1\n",
    "            final_loss = loss + sup_loss * factor\n",
    "            final_loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            train_loss.update(loss.item(), inputs.size(0))\n",
    "            supervised_loss.update(sup_loss.item(), label_batchsize)\n",
    "            \n",
    "            sup_loss_list.append(supervised_loss.avg)\n",
    "            train_loss_list.append(train_loss.avg)\n",
    "        \n",
    "        return sup_loss_list, train_loss_list\n",
    "    \n",
    "    final_sup_loss = []\n",
    "    final_train_loss = []\n",
    "    for i in range(nb_epoch):\n",
    "        print(f'Epoch: {i}')\n",
    "        sup_loss_list, train_loss_list = train()\n",
    "        final_sup_loss.extend(sup_loss_list)\n",
    "        final_train_loss.extend(train_loss_list)\n",
    "        print(train_loss_list)\n",
    "    \n",
    "    return lr_list, final_train_loss, final_sup_loss\n",
    "\n",
    "# Uncomment the following to use lr_finder\n",
    "\n",
    "# lr_list, final_train_loss, final_sup_loss = lr_finder(20)\n",
    "# plt.plot(lr_list, final_train_loss)\n",
    "# plt.grid(True)\n",
    "# plt.title(\"Learning rate Finder\")\n",
    "# plt.xlabel(\"learning rate\")\n",
    "# plt.ylabel('Loss')\n",
    "# plt.show()\n",
    "# plt.plot(lr_list, final_sup_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_rampdown(current, rampdown_length):\n",
    "    \"\"\"Cosine rampdown from https://arxiv.org/abs/1608.03983\"\"\"\n",
    "    assert 0 <= current <= rampdown_length\n",
    "    return float(.5 * (np.cos(np.pi * current / rampdown_length) + 1))\n",
    "\n",
    "\n",
    "def linear_rampup_new(current, x1, y1, x2, y2):\n",
    "    '''functional value of line between (x1, y1) and (x2, y2)'''\n",
    "    \n",
    "    m = (y1-y2)/(x1-x2)\n",
    "    c = (x1*y2-x2*y1)/(x1-x2)\n",
    "    \n",
    "    return m*current + c\n",
    "\n",
    "def sup_factor_schedule(epoch):\n",
    "    if epoch <= 120:\n",
    "        return .1\n",
    "    else:\n",
    "        return linear_rampup_new(epoch, 120, 0.1, 300, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sup_loss_ema = 0 \n",
    "loss_ema = 0\n",
    "def train(epoch, unsupervised=False, sup_factor=.1, ema=False):\n",
    "    global sup_loss_ema, loss_ema\n",
    "    \n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    train_loss = AverageMeter()\n",
    "    supervised_loss = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    batch_time = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    net.train()\n",
    "\n",
    "    end = time.time()\n",
    "    \n",
    "    for batch_idx, (inputs1, inputs2, _, indexes) in enumerate(trainloader):\n",
    "        data_time.update(time.time() - end)\n",
    "        \n",
    "        if unsupervised:\n",
    "            inputs = torch.cat((inputs1, inputs2), 0)\n",
    "            inputs, indexes = inputs.to(device), indexes.to(device)\n",
    "            features, _ = net(inputs)\n",
    "            loss = criterion(features, indexes)\n",
    "            sup_loss = 0\n",
    "            supervised_loss.update(sup_loss, 1) #This step is there so that print doesn't throw any error and I am lazy\n",
    "        else:\n",
    "            try:\n",
    "                labeled_input, target = labeled_dataloader_iter.next()\n",
    "            except:\n",
    "                labeled_dataloader_iter = iter(labeled_dataloader)\n",
    "                labeled_input, target = labeled_dataloader_iter.next()\n",
    "            \n",
    "            label_batchsize = labeled_input.size(0)\n",
    "            inputs = torch.cat((labeled_input, inputs1, inputs2), 0)\n",
    "            inputs, target, indexes = inputs.to(device), target.to(device), indexes.to(device)\n",
    "\n",
    "            features, pred = net(inputs)\n",
    "            features = features[label_batchsize:]\n",
    "            pred = pred[:label_batchsize]\n",
    "\n",
    "            loss = criterion(features, indexes)\n",
    "            sup_loss = supervised_criterion(pred, target)\n",
    "            supervised_loss.update(sup_loss.item(), label_batchsize)\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "#         sup_factor = linear_rampup(epoch, rampup_length=200)\n",
    "        \n",
    "        if ema:\n",
    "            if epoch == 0 and batch_idx==0:\n",
    "                sup_factor = 1\n",
    "            elif epoch == 0 and batch_idx==1:\n",
    "                sup_loss_ema = sup_loss\n",
    "                loss_ema = loss\n",
    "                sup_factor = loss.item()/sup_loss.item()\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    sup_loss_ema = args.alpha * sup_loss_ema + (1-args.alpha) * sup_loss\n",
    "                    loss_ema = args.alpha * loss_ema + (1-args.alpha) * loss\n",
    "                    sup_factor = loss_ema.item() / sup_loss_ema.item()\n",
    "        \n",
    "        final_loss = loss + sup_loss * sup_factor\n",
    "        final_loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss.update(loss.item(), inputs.size(0))\n",
    "        \n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Epoch: [{}][{}/{}] '\n",
    "                  'Time: {batch_time.val:.3f} ({batch_time.avg:.3f}) '\n",
    "                  'Data: {data_time.val:.3f} ({data_time.avg:.3f}) '\n",
    "                  'Loss: {train_loss.val:.4f} ({train_loss.avg:.4f}) '\n",
    "                  'Sup Loss: {supervised_loss.val:.4f} ({supervised_loss.avg:.4f})'.format(\n",
    "                epoch, batch_idx, len(trainloader), batch_time=batch_time, data_time=data_time, train_loss=train_loss,\n",
    "                supervised_loss=supervised_loss))\n",
    "    # add log\n",
    "    writer.add_scalar('Loss/Unsup', train_loss.avg, epoch)\n",
    "    writer.add_scalar('Loss/Sup', supervised_loss.avg, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Training Model..\n",
      "\n",
      "Epoch: 0\n",
      "Epoch: [0][0/99] Time: 11.698 (11.698) Data: 2.516 (2.516) Loss: 7.8726 (7.8726) Sup Loss: 2.6639 (2.6639)\n",
      "Epoch: [0][10/99] Time: 0.285 (1.338) Data: 0.005 (0.237) Loss: 7.4143 (7.6473) Sup Loss: 2.1275 (2.4607)\n",
      "Epoch: [0][20/99] Time: 0.309 (0.839) Data: 0.021 (0.127) Loss: 7.2650 (7.4883) Sup Loss: 2.1279 (2.3694)\n",
      "Epoch: [0][30/99] Time: 0.285 (0.661) Data: 0.005 (0.089) Loss: 7.0448 (7.3519) Sup Loss: 1.9948 (2.2847)\n",
      "Epoch: [0][40/99] Time: 0.301 (0.572) Data: 0.011 (0.069) Loss: 6.9106 (7.2628) Sup Loss: 1.8530 (2.2239)\n",
      "Epoch: [0][50/99] Time: 0.292 (0.517) Data: 0.007 (0.057) Loss: 6.8776 (7.1659) Sup Loss: 1.9958 (2.1747)\n",
      "Epoch: [0][60/99] Time: 0.287 (0.481) Data: 0.005 (0.049) Loss: 6.4628 (7.0740) Sup Loss: 2.0373 (2.1403)\n",
      "Epoch: [0][70/99] Time: 0.289 (0.455) Data: 0.004 (0.043) Loss: 6.3125 (6.9712) Sup Loss: 1.8986 (2.1062)\n",
      "Epoch: [0][80/99] Time: 0.310 (0.436) Data: 0.006 (0.038) Loss: 6.0025 (6.8782) Sup Loss: 1.8064 (2.0723)\n",
      "Epoch: [0][90/99] Time: 0.299 (0.420) Data: 0.004 (0.034) Loss: 5.8643 (6.7897) Sup Loss: 1.6275 (2.0424)\n",
      "----------Evaluation---------\n",
      "Test [100/10000]\tNet Time 0.024 (0.024)\tCls Time 0.002 (0.002)\tTop1: 46.00  Top5: 91.00\n",
      "Test [200/10000]\tNet Time 0.019 (0.022)\tCls Time 0.001 (0.002)\tTop1: 39.50  Top5: 90.50\n",
      "Test [300/10000]\tNet Time 0.018 (0.021)\tCls Time 0.001 (0.001)\tTop1: 39.33  Top5: 89.67\n",
      "Test [400/10000]\tNet Time 0.025 (0.022)\tCls Time 0.001 (0.001)\tTop1: 40.25  Top5: 89.25\n",
      "Test [500/10000]\tNet Time 0.022 (0.022)\tCls Time 0.001 (0.001)\tTop1: 40.20  Top5: 89.40\n",
      "Test [600/10000]\tNet Time 0.025 (0.022)\tCls Time 0.001 (0.001)\tTop1: 40.50  Top5: 89.17\n",
      "Test [700/10000]\tNet Time 0.024 (0.022)\tCls Time 0.001 (0.001)\tTop1: 40.29  Top5: 88.71\n",
      "Test [800/10000]\tNet Time 0.017 (0.022)\tCls Time 0.001 (0.001)\tTop1: 40.38  Top5: 88.25\n",
      "Test [900/10000]\tNet Time 0.022 (0.022)\tCls Time 0.001 (0.001)\tTop1: 41.00  Top5: 88.67\n",
      "Test [1000/10000]\tNet Time 0.022 (0.022)\tCls Time 0.001 (0.001)\tTop1: 41.00  Top5: 88.60\n",
      "Test [1100/10000]\tNet Time 0.021 (0.022)\tCls Time 0.001 (0.001)\tTop1: 41.73  Top5: 88.36\n",
      "Test [1200/10000]\tNet Time 0.018 (0.021)\tCls Time 0.001 (0.001)\tTop1: 41.17  Top5: 88.08\n",
      "Test [1300/10000]\tNet Time 0.022 (0.021)\tCls Time 0.001 (0.001)\tTop1: 40.85  Top5: 88.31\n",
      "Test [1400/10000]\tNet Time 0.022 (0.021)\tCls Time 0.001 (0.001)\tTop1: 40.50  Top5: 88.21\n",
      "Test [1500/10000]\tNet Time 0.020 (0.021)\tCls Time 0.001 (0.001)\tTop1: 40.73  Top5: 88.40\n",
      "Test [1600/10000]\tNet Time 0.022 (0.021)\tCls Time 0.001 (0.001)\tTop1: 40.00  Top5: 88.38\n",
      "Test [1700/10000]\tNet Time 0.024 (0.022)\tCls Time 0.001 (0.001)\tTop1: 40.41  Top5: 87.88\n",
      "Test [1800/10000]\tNet Time 0.019 (0.021)\tCls Time 0.001 (0.001)\tTop1: 40.50  Top5: 87.83\n",
      "Test [1900/10000]\tNet Time 0.019 (0.021)\tCls Time 0.001 (0.001)\tTop1: 40.63  Top5: 87.68\n",
      "Test [2000/10000]\tNet Time 0.022 (0.021)\tCls Time 0.001 (0.001)\tTop1: 40.50  Top5: 87.55\n",
      "Test [2100/10000]\tNet Time 0.024 (0.021)\tCls Time 0.001 (0.001)\tTop1: 40.52  Top5: 87.67\n",
      "Test [2200/10000]\tNet Time 0.021 (0.021)\tCls Time 0.001 (0.001)\tTop1: 40.50  Top5: 87.95\n",
      "Test [2300/10000]\tNet Time 0.017 (0.021)\tCls Time 0.001 (0.001)\tTop1: 40.52  Top5: 88.00\n",
      "Test [2400/10000]\tNet Time 0.019 (0.021)\tCls Time 0.001 (0.001)\tTop1: 40.71  Top5: 88.04\n",
      "Test [2500/10000]\tNet Time 0.025 (0.021)\tCls Time 0.001 (0.001)\tTop1: 40.96  Top5: 87.92\n",
      "Test [2600/10000]\tNet Time 0.028 (0.022)\tCls Time 0.001 (0.001)\tTop1: 40.46  Top5: 87.88\n",
      "Test [2700/10000]\tNet Time 0.023 (0.022)\tCls Time 0.001 (0.001)\tTop1: 40.56  Top5: 87.93\n",
      "Test [2800/10000]\tNet Time 0.023 (0.022)\tCls Time 0.001 (0.001)\tTop1: 40.54  Top5: 87.89\n",
      "Test [2900/10000]\tNet Time 0.020 (0.022)\tCls Time 0.001 (0.001)\tTop1: 40.34  Top5: 87.83\n",
      "Test [3000/10000]\tNet Time 0.023 (0.022)\tCls Time 0.001 (0.001)\tTop1: 40.53  Top5: 87.87\n",
      "Test [3100/10000]\tNet Time 0.019 (0.022)\tCls Time 0.001 (0.001)\tTop1: 40.71  Top5: 87.87\n",
      "Test [3200/10000]\tNet Time 0.023 (0.022)\tCls Time 0.001 (0.001)\tTop1: 40.78  Top5: 87.84\n",
      "Test [3300/10000]\tNet Time 0.018 (0.021)\tCls Time 0.001 (0.001)\tTop1: 40.67  Top5: 87.67\n",
      "Test [3400/10000]\tNet Time 0.021 (0.021)\tCls Time 0.001 (0.001)\tTop1: 40.65  Top5: 87.62\n",
      "Test [3500/10000]\tNet Time 0.020 (0.021)\tCls Time 0.001 (0.001)\tTop1: 40.71  Top5: 87.60\n",
      "Test [3600/10000]\tNet Time 0.024 (0.021)\tCls Time 0.001 (0.001)\tTop1: 40.81  Top5: 87.78\n",
      "Test [3700/10000]\tNet Time 0.020 (0.021)\tCls Time 0.001 (0.001)\tTop1: 40.84  Top5: 87.78\n",
      "Test [3800/10000]\tNet Time 0.024 (0.022)\tCls Time 0.001 (0.001)\tTop1: 40.89  Top5: 87.76\n",
      "Test [3900/10000]\tNet Time 0.026 (0.022)\tCls Time 0.001 (0.001)\tTop1: 41.13  Top5: 87.77\n",
      "Test [4000/10000]\tNet Time 0.023 (0.022)\tCls Time 0.001 (0.001)\tTop1: 41.02  Top5: 87.85\n",
      "Test [4100/10000]\tNet Time 0.023 (0.022)\tCls Time 0.001 (0.001)\tTop1: 40.95  Top5: 87.83\n",
      "Test [4200/10000]\tNet Time 0.025 (0.022)\tCls Time 0.001 (0.001)\tTop1: 41.07  Top5: 87.76\n",
      "Test [4300/10000]\tNet Time 0.024 (0.022)\tCls Time 0.001 (0.001)\tTop1: 41.09  Top5: 87.77\n",
      "Test [4400/10000]\tNet Time 0.024 (0.022)\tCls Time 0.001 (0.001)\tTop1: 41.39  Top5: 87.77\n",
      "Test [4500/10000]\tNet Time 0.023 (0.022)\tCls Time 0.001 (0.001)\tTop1: 41.40  Top5: 87.89\n",
      "Test [4600/10000]\tNet Time 0.027 (0.022)\tCls Time 0.001 (0.001)\tTop1: 41.30  Top5: 87.93\n",
      "Test [4700/10000]\tNet Time 0.023 (0.022)\tCls Time 0.001 (0.001)\tTop1: 41.51  Top5: 88.02\n",
      "Test [4800/10000]\tNet Time 0.022 (0.022)\tCls Time 0.001 (0.001)\tTop1: 41.65  Top5: 88.02\n",
      "Test [4900/10000]\tNet Time 0.026 (0.022)\tCls Time 0.001 (0.001)\tTop1: 41.49  Top5: 87.88\n",
      "Test [5000/10000]\tNet Time 0.023 (0.022)\tCls Time 0.001 (0.001)\tTop1: 41.58  Top5: 87.86\n",
      "Test [5100/10000]\tNet Time 0.024 (0.022)\tCls Time 0.001 (0.001)\tTop1: 41.65  Top5: 87.94\n",
      "Test [5200/10000]\tNet Time 0.024 (0.022)\tCls Time 0.001 (0.001)\tTop1: 41.69  Top5: 87.92\n",
      "Test [5300/10000]\tNet Time 0.024 (0.022)\tCls Time 0.001 (0.001)\tTop1: 41.58  Top5: 87.89\n",
      "Test [5400/10000]\tNet Time 0.024 (0.022)\tCls Time 0.001 (0.001)\tTop1: 41.41  Top5: 87.78\n",
      "Test [5500/10000]\tNet Time 0.025 (0.022)\tCls Time 0.001 (0.001)\tTop1: 41.38  Top5: 87.73\n",
      "Test [5600/10000]\tNet Time 0.027 (0.022)\tCls Time 0.001 (0.001)\tTop1: 41.34  Top5: 87.80\n",
      "Test [5700/10000]\tNet Time 0.023 (0.022)\tCls Time 0.001 (0.001)\tTop1: 41.28  Top5: 87.82\n",
      "Test [5800/10000]\tNet Time 0.024 (0.022)\tCls Time 0.001 (0.001)\tTop1: 41.36  Top5: 87.95\n",
      "Test [5900/10000]\tNet Time 0.022 (0.022)\tCls Time 0.001 (0.001)\tTop1: 41.31  Top5: 87.98\n",
      "Test [6000/10000]\tNet Time 0.026 (0.023)\tCls Time 0.001 (0.001)\tTop1: 41.27  Top5: 88.00\n",
      "Test [6100/10000]\tNet Time 0.021 (0.022)\tCls Time 0.001 (0.001)\tTop1: 41.26  Top5: 87.98\n",
      "Test [6200/10000]\tNet Time 0.023 (0.022)\tCls Time 0.001 (0.001)\tTop1: 41.31  Top5: 87.94\n",
      "Test [6300/10000]\tNet Time 0.022 (0.022)\tCls Time 0.001 (0.001)\tTop1: 41.25  Top5: 87.90\n",
      "Test [6400/10000]\tNet Time 0.022 (0.022)\tCls Time 0.001 (0.001)\tTop1: 41.14  Top5: 87.94\n",
      "Test [6500/10000]\tNet Time 0.024 (0.023)\tCls Time 0.001 (0.001)\tTop1: 41.08  Top5: 87.98\n",
      "Test [6600/10000]\tNet Time 0.022 (0.022)\tCls Time 0.001 (0.001)\tTop1: 41.09  Top5: 88.00\n",
      "Test [6700/10000]\tNet Time 0.023 (0.023)\tCls Time 0.001 (0.001)\tTop1: 41.07  Top5: 88.09\n",
      "Test [6800/10000]\tNet Time 0.023 (0.023)\tCls Time 0.001 (0.001)\tTop1: 40.97  Top5: 88.06\n",
      "Test [6900/10000]\tNet Time 0.020 (0.022)\tCls Time 0.001 (0.001)\tTop1: 40.96  Top5: 88.03\n",
      "Test [7000/10000]\tNet Time 0.026 (0.023)\tCls Time 0.001 (0.001)\tTop1: 41.04  Top5: 88.09\n",
      "Test [7100/10000]\tNet Time 0.029 (0.023)\tCls Time 0.001 (0.001)\tTop1: 41.07  Top5: 88.11\n",
      "Test [7200/10000]\tNet Time 0.024 (0.023)\tCls Time 0.001 (0.001)\tTop1: 41.12  Top5: 88.12\n",
      "Test [7300/10000]\tNet Time 0.019 (0.023)\tCls Time 0.001 (0.001)\tTop1: 41.07  Top5: 88.15\n",
      "Test [7400/10000]\tNet Time 0.023 (0.023)\tCls Time 0.001 (0.001)\tTop1: 41.05  Top5: 88.19\n",
      "Test [7500/10000]\tNet Time 0.025 (0.023)\tCls Time 0.001 (0.001)\tTop1: 40.99  Top5: 88.16\n",
      "Test [7600/10000]\tNet Time 0.024 (0.023)\tCls Time 0.001 (0.001)\tTop1: 40.93  Top5: 88.07\n",
      "Test [7700/10000]\tNet Time 0.023 (0.023)\tCls Time 0.001 (0.001)\tTop1: 40.96  Top5: 88.03\n",
      "Test [7800/10000]\tNet Time 0.023 (0.023)\tCls Time 0.001 (0.001)\tTop1: 41.00  Top5: 88.05\n",
      "Test [7900/10000]\tNet Time 0.024 (0.023)\tCls Time 0.001 (0.001)\tTop1: 40.91  Top5: 88.05\n",
      "Test [8000/10000]\tNet Time 0.024 (0.023)\tCls Time 0.001 (0.001)\tTop1: 40.88  Top5: 87.96\n",
      "Test [8100/10000]\tNet Time 0.023 (0.023)\tCls Time 0.001 (0.001)\tTop1: 40.91  Top5: 87.95\n",
      "Test [8200/10000]\tNet Time 0.024 (0.023)\tCls Time 0.001 (0.001)\tTop1: 40.83  Top5: 87.94\n",
      "Test [8300/10000]\tNet Time 0.024 (0.023)\tCls Time 0.001 (0.001)\tTop1: 40.76  Top5: 87.92\n",
      "Test [8400/10000]\tNet Time 0.025 (0.023)\tCls Time 0.001 (0.001)\tTop1: 40.71  Top5: 87.93\n",
      "Test [8500/10000]\tNet Time 0.023 (0.023)\tCls Time 0.001 (0.001)\tTop1: 40.61  Top5: 87.92\n",
      "Test [8600/10000]\tNet Time 0.025 (0.023)\tCls Time 0.001 (0.001)\tTop1: 40.63  Top5: 87.93\n",
      "Test [8700/10000]\tNet Time 0.025 (0.023)\tCls Time 0.002 (0.001)\tTop1: 40.63  Top5: 87.98\n",
      "Test [8800/10000]\tNet Time 0.025 (0.023)\tCls Time 0.001 (0.001)\tTop1: 40.68  Top5: 87.94\n",
      "Test [8900/10000]\tNet Time 0.023 (0.023)\tCls Time 0.001 (0.001)\tTop1: 40.79  Top5: 87.85\n",
      "Test [9000/10000]\tNet Time 0.023 (0.023)\tCls Time 0.001 (0.001)\tTop1: 40.84  Top5: 87.82\n",
      "Test [9100/10000]\tNet Time 0.024 (0.023)\tCls Time 0.001 (0.001)\tTop1: 40.84  Top5: 87.87\n",
      "Test [9200/10000]\tNet Time 0.023 (0.023)\tCls Time 0.001 (0.001)\tTop1: 40.89  Top5: 87.89\n",
      "Test [9300/10000]\tNet Time 0.024 (0.023)\tCls Time 0.001 (0.001)\tTop1: 40.91  Top5: 87.87\n",
      "Test [9400/10000]\tNet Time 0.025 (0.023)\tCls Time 0.001 (0.001)\tTop1: 40.94  Top5: 87.88\n",
      "Test [9500/10000]\tNet Time 0.024 (0.023)\tCls Time 0.001 (0.001)\tTop1: 40.97  Top5: 87.93\n",
      "Test [9600/10000]\tNet Time 0.025 (0.023)\tCls Time 0.001 (0.001)\tTop1: 40.95  Top5: 87.92\n",
      "Test [9700/10000]\tNet Time 0.024 (0.023)\tCls Time 0.001 (0.001)\tTop1: 40.90  Top5: 87.95\n",
      "Test [9800/10000]\tNet Time 0.025 (0.023)\tCls Time 0.001 (0.001)\tTop1: 40.86  Top5: 87.97\n",
      "Test [9900/10000]\tNet Time 0.025 (0.023)\tCls Time 0.001 (0.001)\tTop1: 40.82  Top5: 87.98\n",
      "Test [10000/10000]\tNet Time 0.028 (0.023)\tCls Time 0.001 (0.001)\tTop1: 40.80  Top5: 88.01\n",
      "40.8\n",
      "Evaluation Time:17.64\n",
      "Saving..\n",
      "accuracy: 40.8% \t (best acc: 40.8%)\n"
     ]
    }
   ],
   "source": [
    "print('==> Training Model..')\n",
    "best_acc = 0  # best test accuracy\n",
    "test_log_file = open(args.log_dir + suffix + '.txt', \"w\")\n",
    "test_epoch = 30\n",
    "\n",
    "def trns1(x): # this is more traditionally used \n",
    "    distance = (1-x)/2\n",
    "    return math.exp(-(distance**2)/.01)\n",
    "\n",
    "for epoch in range(300):\n",
    "\n",
    "    # training\n",
    "#     sup_factor = cosine_rampdown(epoch, 130)\n",
    "    sup_factor = sup_factor_schedule(epoch)\n",
    "    train(epoch, unsupervised=False, sup_factor=sup_factor, ema=False)\n",
    "\n",
    "    # testing every 10 epochs\n",
    "    if epoch % test_epoch == 0 or epoch == 299:\n",
    "        net.eval()\n",
    "        print('----------Evaluation---------')\n",
    "        start = time.time()\n",
    "\n",
    "        acc = kNN(epoch, net, trainloader, testloader, 200, args.batch_t, len(trainset), low_dim=args.low_dim)\n",
    "        print(f\"Evaluation Time:{time.time() - start:.2f}\")\n",
    "        writer.add_scalar('Accuracy/nn_acc', acc, epoch)\n",
    "\n",
    "        if acc > best_acc:\n",
    "            print('Saving..')\n",
    "            state = {\n",
    "                'net': net.state_dict(),\n",
    "                'acc': acc,\n",
    "                'epoch': epoch,\n",
    "            }\n",
    "            if not os.path.isdir(args.model_dir):\n",
    "                os.mkdir(args.model_dir)\n",
    "            torch.save(state, args.model_dir + suffix + '.t')\n",
    "            best_acc = acc\n",
    "\n",
    "        print(f'accuracy: {acc:.1f}% \\t (best acc: {best_acc:.1f}%)')\n",
    "        print(f'[Epoch]: {epoch}', file=test_log_file)\n",
    "        print(f'accuracy: {acc:.2f}% \\t (best acc: {best_acc:.2f}%)', file=test_log_file)\n",
    "        \n",
    "        \n",
    "        # Plot the UMAP \n",
    "        feature_mat, label_arr = sem_sup_feature(net, combined_dataloader)\n",
    "        reducer = umap.UMAP(metric = \"cosine\", spread = .1)\n",
    "        feature_and_weight = np.concatenate([feature_mat, net.linear_class.weight.data.cpu().numpy()])\n",
    "        embedding = reducer.fit_transform(feature_and_weight)\n",
    "        \n",
    "        \n",
    "        label_embedding = embedding[label_index]\n",
    "        unlabel_embedding = embedding[unlabel_index]\n",
    "        weight_embedding = embedding[-10:]\n",
    "\n",
    "\n",
    "        fig = plt.figure(figsize=(7,7))\n",
    "        plt.scatter(unlabel_embedding[:,0], unlabel_embedding[:,1], s=1, alpha = 0.8, \n",
    "                    c = label_arr[unlabel_index], cmap=\"tab10\")\n",
    "        plt.grid(True)\n",
    "        plt.tick_params(axis='x', colors=(0,0,0,0))\n",
    "        plt.tick_params(axis='y', colors=(0,0,0,0))\n",
    "        plt.scatter(label_embedding[:,0], label_embedding[:,1], marker='o', color='b', alpha=.4)\n",
    "        plt.scatter(weight_embedding[:,0], weight_embedding[:,1], marker='o', color='r', alpha=.7)\n",
    "        plt.title(\"UMAP -- for trainable weight\")\n",
    "        # plt.savefig('temp')\n",
    "\n",
    "        canvas = FigureCanvas(fig)\n",
    "        ax = fig.gca()\n",
    "\n",
    "        canvas.draw()       # draw the canvas, cache the renderer\n",
    "        width, height = fig.get_size_inches() * fig.get_dpi()\n",
    "        width, height = int(width), int(height)\n",
    "        img = np.fromstring(canvas.tostring_rgb(), dtype='uint8').reshape(height, width, 3)\n",
    "        writer.add_image('UMAP', img, epoch, dataformats = \"HWC\")\n",
    "        \n",
    "        # Do Label Propagation \n",
    "        \n",
    "        Mat_Label = feature_mat[label_index] # Labelled data matrix\n",
    "        labels = label_arr[label_index] # Corresponding labels of Labelled data matrix\n",
    "\n",
    "        Mat_Unlabel = feature_mat[unlabel_index] # UnLabelled data matrix\n",
    "        rest_label  = label_arr[unlabel_index] # Rest of the lable, won't be used\n",
    "        \n",
    "        # Stacking features from labelled examples at the begining\n",
    "        MatX = np.vstack((Mat_Label, Mat_Unlabel))\n",
    "        MatX = MatX / np.linalg.norm(MatX, axis=-1)[:, np.newaxis] # not really required since norm of feature is already 1\n",
    "\n",
    "        affinity_matrix = buildGraph(MatX, trns1, 150) # creating sparse affinity matrix\n",
    "        affinity_matrix_time = time.time()\n",
    "        unlabel_data_labels = labelPropagation(affinity_matrix, Mat_Label, Mat_Unlabel, labels, alpha=.95) # Doing LP \n",
    "\n",
    "        accuracy = get_acc(unlabel_data_labels, rest_label)\n",
    "        \n",
    "        print(f'Label Propagation accuracy is {accuracy:.4f}', file=test_log_file)\n",
    "        test_log_file.flush()\n",
    "        writer.add_scalar('Accuracy/LP_acc', accuracy, epoch)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_mat, label_arr = sem_sup_feature(net, combined_dataloader)\n",
    "Mat_Label = feature_mat[label_index] #Labelled data matrix\n",
    "labels = label_arr[label_index] #Corresponding labels of Labelled data matrix\n",
    "\n",
    "Mat_Unlabel = feature_mat[unlabel_index] #UnLabelled data matrix\n",
    "rest_label  = label_arr[unlabel_index] #Rest of the lable, won't be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.7880 and time for affinity matrix 2 seconds, label propagation time 2 seconds\n"
     ]
    }
   ],
   "source": [
    "# Example of how to do label propagation\n",
    "start_time = time.time()\n",
    "# Two types of disnatce function\n",
    "def trns(x): # taken from this paper https://arxiv.org/pdf/1904.04717.pdf\n",
    "    return 0 if x < 0 else x**9\n",
    "\n",
    "def trns1(x): # this is more traditionally used \n",
    "    distance = (1-x)/2\n",
    "    return math.exp(-(distance**2)/.01)\n",
    "\n",
    "#Stacking features from labelled examples at the begining\n",
    "MatX = np.vstack((Mat_Label, Mat_Unlabel))\n",
    "MatX = MatX / np.linalg.norm(MatX, axis=-1)[:, np.newaxis] # not really required since norm of feature is already 1\n",
    "\n",
    "affinity_matrix = buildGraph(MatX, trns, 20) # creating sparse affinity matrix\n",
    "affinity_matrix_time = time.time()\n",
    "unlabel_data_labels = labelPropagation(affinity_matrix, Mat_Label, Mat_Unlabel, labels, alpha=.95) # Doing LP \n",
    "\n",
    "accuracy = get_acc(unlabel_data_labels, rest_label) # Measuring accuracy \n",
    "time_taken = time.time() - start_time\n",
    "print(f\"Accuracy is {accuracy:.4f} and time for affinity matrix {affinity_matrix_time - start_time:.0f} seconds, label propagation time {time.time() - affinity_matrix_time:.0f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_mat, label_arr = sem_sup_feature(net, combined_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import umap\n",
    "# reducer = umap.UMAP(metric = \"cosine\", spread = .1)\n",
    "# feature_and_weight = np.concatenate([feature_mat[unlabel_index], net.linear_class.weight.data.cpu().numpy()])\n",
    "# embedding = reducer.fit_transform(feature_and_weight)\n",
    "\n",
    "# unlabel_embedding = embedding[:len(unlabel_index)]\n",
    "# label_embedding = embedding[[unlabel_index.index(i) for i in label_index]]\n",
    "# weight_embedding = embedding[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=(7,7))\n",
    "# plt.scatter(unlabel_embedding[:,0], unlabel_embedding[:,1], s=1, alpha = 0.8, \n",
    "#             c = label_arr[unlabel_index], cmap=\"tab10\")\n",
    "# plt.grid(True)\n",
    "# plt.tick_params(axis='x', colors=(0,0,0,0))\n",
    "# plt.tick_params(axis='y', colors=(0,0,0,0))\n",
    "# plt.scatter(label_embedding[:,0], label_embedding[:,1], marker='o', color='b', alpha=.4)\n",
    "# plt.scatter(weight_embedding[:,0], weight_embedding[:,1], marker='o', color='r', alpha=.7)\n",
    "# plt.title(\"UMAP -- for trainable weight\")\n",
    "# # plt.savefig('temp')\n",
    "\n",
    "# canvas = FigureCanvas(fig)\n",
    "# ax = fig.gca()\n",
    "\n",
    "# # ax.text(0.0,0.0,\"Test\", fontsize=45)\n",
    "# # ax.axis('off')\n",
    "\n",
    "# canvas.draw()       # draw the canvas, cache the renderer\n",
    "# width, height = fig.get_size_inches() * fig.get_dpi()\n",
    "# width, height = int(width), int(height)\n",
    "# img = np.fromstring(canvas.tostring_rgb(), dtype='uint8').reshape(height, width, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch] *",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}