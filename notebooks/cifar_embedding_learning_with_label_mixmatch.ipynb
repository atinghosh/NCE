{
 "cells": [
  {
   "source": [
    "#Most of the code is copied from https://github.com/mangye16/Unsupervised_Embedding_Learning\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "import math\n",
    "from easydict import EasyDict as edict\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import umap\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "from matplotlib.figure import Figure\n",
    "\n",
    "from scipy.sparse.linalg import cg\n",
    "from scipy.sparse import csr_matrix, identity, diags\n",
    "from scipy.stats import entropy\n",
    "\n",
    "import faiss\n",
    "from progressbar import ProgressBar\n",
    "from scipy.stats import bernoulli\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "# Hard location of path \n",
    "# Labels = \"../labels/cifar10/250_balanced_labels/00.txt\" \n",
    "suffix = 'cifar_mixmatch' # Change this suffix to a suitable name for the experiment for tensorboard and logging"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 4
  },
  {
   "source": [
    "def generate_subset_of_CIFAR_for_ssl(samples_per_class, label_sample_per_class=25, seed=1):\n",
    "    '''Generate label and unalabel index for CIFAR10, total unlabel index is same as no of samples\n",
    "    \n",
    "    Args:\n",
    "        \n",
    "        samples_per_class(int): no. of images to be considered per class, maximum posisble is 5000\n",
    "        \n",
    "        label_sample_per_class(int): no. of label images per class, must be less than equal to samples_per_class, \n",
    "        default is 25\n",
    "        \n",
    "    \n",
    "    Returns:\n",
    "        index for unlabel and label\n",
    "    '''\n",
    " \n",
    "    trainset_cifar = datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "    trainloader = torch.utils.data.DataLoader(trainset_cifar, batch_size=1024, shuffle=False, num_workers=20)\n",
    "    \n",
    "    list_of_target = []\n",
    "    for i, (_, target) in enumerate(trainloader):\n",
    "        list_of_target.append(target)\n",
    "    list_of_target = torch.cat(list_of_target).numpy()\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    sampled_index = []\n",
    "    index_for_label = []\n",
    "    for i in range(10):\n",
    "        ii = np.where(list_of_target == i)[0]\n",
    "        cls_sample = list(np.random.choice(ii, samples_per_class, replace=False))\n",
    "        sampled_index.extend(cls_sample)\n",
    "        cls_sample_labeled = list(np.random.choice(cls_sample, label_sample_per_class, replace=False))\n",
    "        index_for_label.extend(cls_sample_labeled)\n",
    "    \n",
    "    sampled_index = list(set(sampled_index)-set(index_for_label))\n",
    "    return sampled_index, index_for_label\n",
    "\n",
    "unlabel_index, label_index = generate_subset_of_CIFAR_for_ssl(5000, 25, 1)"
   ],
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Files already downloaded and verified\n"
    }
   ],
   "metadata": {},
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch-t: softmax temperature parameter (0.05-0.1)\n",
    "# low-dim: the feature embedding dimension (default: 128)\n",
    "\n",
    "args = edict({'dataset':'cifar', 'lr': .05, 'resume': '', 'log_dir': 'log/', 'model_dir': 'checkpoint/',\n",
    "              'test_epoch': 1, 'low_dim': 128, 'batch_t': .1, 'batch_m': 1, 'batch_size': 500, 'gpu': '0, 1, 2',\n",
    "              'alpha': .8 }) \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "vis_log_dir = args.log_dir + suffix + '/'\n",
    "if not os.path.isdir(vis_log_dir):\n",
    "    os.makedirs(vis_log_dir)\n",
    "writer = SummaryWriter(log_dir=vis_log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resnet Model\n",
    "\n",
    "class Normalize(nn.Module):\n",
    "\n",
    "    def __init__(self, power=2, temp=1):\n",
    "        super(Normalize, self).__init__()\n",
    "        self.power = power\n",
    "        self.temp = temp\n",
    "    \n",
    "    def forward(self, x):\n",
    "        norm = x.pow(self.power).sum(1, keepdim=True).pow(1./self.power)\n",
    "        out = x.div(norm)\n",
    "        return out/self.temp\n",
    "    \n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, pool_len =4, low_dim=128, fixed_classifier=False, temp=1):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.temp = temp\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear_embedding = nn.Linear(512*block.expansion, low_dim)\n",
    "        self.linear_class = nn.Linear(low_dim, 10, bias=False)\n",
    "        if fixed_classifier:\n",
    "            M = np.random.normal(0, 1, size=(low_dim, low_dim))\n",
    "            ortho, _, _ = np.linalg.svd(M)\n",
    "            ortho = ortho[:10]\n",
    "            self.linear_class.weight.data = torch.tensor(ortho, dtype=torch.float).cuda()\n",
    "            self.linear_class.weight.detach_()\n",
    "        # with torch.no_grad():\n",
    "        #     self.linear_class.weight.div_(torch.norm(self.linear_class.weight, dim=1, keepdim=True))\n",
    "        self.l2norm_for_feature = Normalize()\n",
    "        # self.l2norm_for_weight = Normalize(temp=.1)\n",
    "        self.pool_len = pool_len\n",
    "        # w = torch.nn.Parameter(torch.randn(128, 10))\n",
    "        # for m in self.modules():\n",
    "            # if isinstance(m, nn.Conv2d):\n",
    "                # n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                # m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            # elif isinstance(m, nn.BatchNorm2d):\n",
    "                # m.weight.data.fill_(1)\n",
    "                # m.bias.data.zero_()\n",
    "                \n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, self.pool_len)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out_embedding = self.linear_embedding(out)\n",
    "        out_embedding = self.l2norm_for_feature(out_embedding)\n",
    "        # with torch.no_grad():\n",
    "        #     self.linear_class.weight.div_(torch.norm(self.linear_class.weight, dim=1, keepdim=True))\n",
    "        # self.linear_class.weight.div_(torch.norm(self.linear_class.weight, dim=1, keepdim=True))\n",
    "        # type(self.l2norm_for_weight(self.linear_class.weight))\n",
    "        # self.linear_class.weight = self.l2norm_for_weight(self.linear_class.weight)\n",
    "        # self.linear_class.weight = torch.nn.Parameter(10 * self.linear_class.weight/torch.norm(self.linear_class.weight, dim=1, keepdim=True))\n",
    "        # out_class = 10*self.linear_class(out_embedding)/torch.norm(self.linear_class.weight, dim=1, keepdim=True).transpose(1,0)\n",
    "        out_class = self.linear_class(out_embedding)\n",
    "        return out_embedding, out_class/self.temp\n",
    "        # return out_embedding, out_class\n",
    "\n",
    "    # def normalize_weight(self):\n",
    "    #     with torch.no_grad():\n",
    "    #         self.linear_class.weight.div_(torch.norm(self.linear_class.weight, dim=1, keepdim=True))\n",
    "\n",
    "\n",
    "\n",
    "def ResNet18(pool_len = 4, low_dim=128, fixed_weight=True, temperature=1):\n",
    "    return ResNet(BasicBlock, [2,2,2,2], pool_len, low_dim, fixed_classifier= fixed_weight, temp=temperature)\n",
    "\n",
    "\n",
    "class BatchCriterion(nn.Module):  # Unsupervised Loss\n",
    "    ''' Compute the unsupervised loss within each batch  \n",
    "    '''\n",
    "    def __init__(self, T, batchSize):\n",
    "        super(BatchCriterion, self).__init__()\n",
    "        self.T = T\n",
    "        self.diag_mat = 1 - torch.eye(batchSize*2).cuda()\n",
    "        \n",
    "    def forward(self, x, targets):\n",
    "        batchSize = x.size(0)\n",
    "        \n",
    "        #get positive innerproduct\n",
    "        reordered_x = torch.cat((x.narrow(0,batchSize//2,batchSize//2), x.narrow(0,0,batchSize//2)), 0)\n",
    "        #reordered_x = reordered_x.data\n",
    "        pos = (x*reordered_x.data).sum(1).div_(self.T).exp_()\n",
    "\n",
    "        #get all innerproduct, remove diag\n",
    "        all_prob = torch.mm(x,x.t().data).div_(self.T).exp_()*self.diag_mat\n",
    "        all_div = all_prob.sum(1)\n",
    "        \n",
    "\n",
    "        lnPmt = torch.div(pos, all_div)\n",
    "\n",
    "        # negative probability\n",
    "        Pon_div = all_div.repeat(batchSize,1)\n",
    "        lnPon = torch.div(all_prob, Pon_div.t())\n",
    "        lnPon = -lnPon.add(-1)\n",
    "        \n",
    "        # equation 7 in ref. A (NCE paper)\n",
    "        lnPon.log_()\n",
    "        # also remove the pos term\n",
    "        lnPon = lnPon.sum(1) - (-lnPmt.add(-1)).log_()\n",
    "        lnPmt.log_()\n",
    "\n",
    "        lnPmtsum = lnPmt.sum(0)\n",
    "        lnPonsum = lnPon.sum(0)\n",
    "\n",
    "        loss = - (lnPmtsum + lnPonsum)/batchSize\n",
    "        return loss\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed at 120, 160 and 200\"\"\"\n",
    "    lr = args.lr\n",
    "    if epoch >= 120 and epoch < 160:\n",
    "        lr = args.lr * 0.1\n",
    "    elif epoch >= 160 and epoch < 200:\n",
    "        lr = args.lr * 0.05\n",
    "    elif epoch >= 200:\n",
    "        lr = args.lr * 0.01\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    writer.add_scalar('lr', lr, epoch)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10Instance(datasets.CIFAR10):\n",
    "    \"\"\"CIFAR10Instance Dataset.\n",
    "    \"\"\"\n",
    "    def __getitem__(self, index):\n",
    "        if self.train:\n",
    "            img, target = self.data[index], self.targets[index]\n",
    "        else:\n",
    "            img, target = self.data[index], self.targets[index]\n",
    "\n",
    "        # doing this so that it is consistent with all other datasets\n",
    "        # to return a PIL Image\n",
    "        img = Image.fromarray(img)\n",
    "        \n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "            \n",
    "        if self.transform is not None:\n",
    "            img1 = self.transform(img)\n",
    "            if self.train:\n",
    "                img2 = self.transform(img)\n",
    "\n",
    "        if self.train:\n",
    "            return img1, img2, target, index\n",
    "        else:\n",
    "            return img1, target, index\n",
    "\n",
    "\n",
    "class Cutout(object):\n",
    "    \"\"\"Randomly mask out one or more patches from an image.\n",
    "    Args:\n",
    "        n_holes (int): Number of patches to cut out of each image.\n",
    "        length (int): The length (in pixels) of each square patch.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_holes, length):\n",
    "        self.n_holes = n_holes\n",
    "        self.length = length\n",
    "\n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (Tensor): Tensor image of size (C, H, W).\n",
    "        Returns:\n",
    "            Tensor: Image with n_holes of dimension length x length cut out of it.\n",
    "        \"\"\"\n",
    "        h = img.size(1)\n",
    "        w = img.size(2)\n",
    "\n",
    "        mask = np.ones((h, w), np.float32)\n",
    "\n",
    "        for n in range(self.n_holes):\n",
    "            y = np.random.randint(h)\n",
    "            x = np.random.randint(w)\n",
    "\n",
    "            y1 = np.clip(y - self.length // 2, 0, h)\n",
    "            y2 = np.clip(y + self.length // 2, 0, h)\n",
    "            x1 = np.clip(x - self.length // 2, 0, w)\n",
    "            x2 = np.clip(x + self.length // 2, 0, w)\n",
    "\n",
    "            mask[y1: y2, x1: x2] = 0.\n",
    "\n",
    "        mask = torch.from_numpy(mask)\n",
    "        mask = mask.expand_as(img)\n",
    "        img = img * mask\n",
    "\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Utility\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\" \n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "                   \n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0 \n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def kNN(epoch, net, trainloader, testloader, K, sigma, ndata, low_dim=128):\n",
    "    net.eval()\n",
    "    net_time = AverageMeter()\n",
    "    cls_time = AverageMeter()\n",
    "    total = 0\n",
    "    correct_t = 0\n",
    "    testsize = testloader.dataset.__len__()\n",
    "\n",
    "    if hasattr(trainloader.dataset, 'imgs'):\n",
    "        trainLabels = torch.LongTensor([y for (p, y) in trainloader.dataset.imgs]).cuda()\n",
    "    else:\n",
    "        try:\n",
    "            trainLabels = torch.LongTensor(trainloader.dataset.train_labels).cuda()\n",
    "        except:\n",
    "            trainLabels = torch.LongTensor(trainloader.dataset.targets).cuda()\n",
    "    trainFeatures = np.zeros((low_dim, ndata))\n",
    "    C = trainLabels.max() + 1\n",
    "    C = np.int(C)\n",
    "    with torch.no_grad():\n",
    "        transform_bak = trainloader.dataset.transform\n",
    "        trainloader.dataset.transform = testloader.dataset.transform\n",
    "        temploader = torch.utils.data.DataLoader(trainloader.dataset, batch_size=100, shuffle=False, num_workers=10)\n",
    "        for batch_idx, (inputs, _, targets, indexes) in enumerate(temploader):\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            batchSize = inputs.size(0)\n",
    "            features, _ = net(inputs)\n",
    "            #\n",
    "            trainFeatures[:, batch_idx * batchSize:batch_idx * batchSize + batchSize] = features.data.t().cpu().numpy()\n",
    "\n",
    "    trainloader.dataset.transform = transform_bak\n",
    "    #\n",
    "\n",
    "    trainFeatures = torch.Tensor(trainFeatures).cuda()\n",
    "    top1 = 0.\n",
    "    top5 = 0.\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        retrieval_one_hot = torch.zeros(K, C).cuda()\n",
    "        for batch_idx, (inputs, targets, indexes) in enumerate(testloader):\n",
    "            end = time.time()\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            batchSize = inputs.size(0)\n",
    "            features, _ = net(inputs)\n",
    "            total += targets.size(0)\n",
    "\n",
    "            net_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            dist = torch.mm(features, trainFeatures)\n",
    "            yd, yi = dist.topk(K, dim=1, largest=True, sorted=True)\n",
    "            candidates = trainLabels.view(1, -1).expand(batchSize, -1)\n",
    "            retrieval = torch.gather(candidates, 1, yi)\n",
    "\n",
    "            retrieval_one_hot.resize_(batchSize * K, C).zero_()\n",
    "            retrieval_one_hot.scatter_(1, retrieval.view(-1, 1), 1)\n",
    "            yd_transform = yd.clone().div_(sigma).exp_()\n",
    "            probs = torch.sum(torch.mul(retrieval_one_hot.view(batchSize, -1, C), yd_transform.view(batchSize, -1, 1)),\n",
    "                              1)\n",
    "            _, predictions = probs.sort(1, True)\n",
    "\n",
    "            # Find which predictions match the target\n",
    "            correct = predictions.eq(targets.data.view(-1, 1))\n",
    "            cls_time.update(time.time() - end)\n",
    "\n",
    "            top1 = top1 + correct.narrow(1, 0, 1).sum().item()\n",
    "            top5 = top5 + correct.narrow(1, 0, 5).sum().item()\n",
    "\n",
    "            print('Test [{}/{}]\\t'\n",
    "                  'Net Time {net_time.val:.3f} ({net_time.avg:.3f})\\t'\n",
    "                  'Cls Time {cls_time.val:.3f} ({cls_time.avg:.3f})\\t'\n",
    "                  'Top1: {:.2f}  Top5: {:.2f}'.format(\n",
    "                total, testsize, top1 * 100. / total, top5 * 100. / total, net_time=net_time, cls_time=cls_time))\n",
    "\n",
    "    print(top1 * 100. / total)\n",
    "\n",
    "    return top1 * 100. / total\n",
    "\n",
    "def return_train_labels_index(Labels_loc):\n",
    "    label_list = []\n",
    "    with open(Labels_loc, 'r') as df:\n",
    "        for l in df.readlines():\n",
    "            a = l.rstrip().split(\"_\")\n",
    "            label_list.append(int(a[0]))\n",
    "\n",
    "    return np.array(label_list)\n",
    "\n",
    "def linear_rampup(current, rampup_length):\n",
    "    \"\"\"Linear rampup\"\"\"\n",
    "    assert current >= 0 and rampup_length >= 0\n",
    "    if current >= rampup_length:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return current / rampup_length\n",
    "    \n",
    "def sem_sup_feature(conv_model: nn.Module, dl: torch.utils.data.dataloader):\n",
    "    '''Extracts features from images using pretrained model\n",
    "\n",
    "    Args:\n",
    "        dl: dataloader\n",
    "        conv_model: pretrained nn.Module object\n",
    "\n",
    "    Returns:\n",
    "        Tuple[list1, list2]\n",
    "        list1: 2D torch tensor of size (no. of observation, embedding dim)\n",
    "        list2: 1D array of corresponding labels\n",
    "    '''\n",
    "    conv_model.eval()\n",
    "    semi_supervised_feature_list = []\n",
    "    label_list = []\n",
    "    for b in dl:\n",
    "        data, label = b\n",
    "        data = data.cuda()\n",
    "        with torch.no_grad():\n",
    "            out, _ = conv_model(data)\n",
    "            b_size = out\n",
    "            semi_supervised_feature_list.append(out)\n",
    "            label_list.append(label)\n",
    "    final_list = torch.cat(semi_supervised_feature_list, dim=0)\n",
    "    final_label_list = torch.cat(label_list, dim=0)\n",
    "    return final_list.cpu().numpy(), final_label_list.cpu().numpy()\n",
    "\n",
    "#create data loaders\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                           download=True, transform=transform_test)\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                            download=True, transform=transform_test)\n",
    "\n",
    "combined_dataset = torch.utils.data.ConcatDataset([trainset, testset])\n",
    "combined_dataloader = torch.utils.data.DataLoader(combined_dataset, batch_size=1024, shuffle=False, num_workers=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildGraph(MatX, trns, knn_num_neighbors=20, dim=128) -> csr_matrix:\n",
    "    '''creates the affinity matrix from the fearure matrix\n",
    "    \n",
    "    MatX has a particular structure. \n",
    "    \n",
    "    Args:\n",
    "        Matx: feature matrix of shape (num_observation, embedding_dimension). It must have the following structure.\n",
    "        The first 250 rows (assuming our SSL is trained on 250 labels) will have features corresponding to 250\n",
    "        labelled examples and remaining rows will be features for unlabelled examples.\n",
    "        \n",
    "        trns(function object): transformation for every element of affinity matrix e.g. lambda x: 0 if x < 0 else x**4\n",
    "        knn_num_neighbors: # of nearest neighbors\n",
    "        \n",
    "        dim: embedding dimension\n",
    "    \n",
    "    Returns:\n",
    "        sparse affinity matrix to be used for label propagation later, where labelled examples are stacked at the\n",
    "        front rows. This is required for label propagation function to work properly. \n",
    "    '''\n",
    "    \n",
    "    num_samples = MatX.shape[0]\n",
    "    index = faiss.IndexFlatIP(dim)\n",
    "    index.add(MatX)\n",
    "    distances, indices = index.search(MatX, knn_num_neighbors)\n",
    "\n",
    "    trns = np.vectorize(trns)\n",
    "\n",
    "    row = np.repeat(np.arange(num_samples), knn_num_neighbors)\n",
    "    col = indices.flatten()\n",
    "    data = distances.flatten()\n",
    "    data = trns(data)\n",
    "    sp_affinity_matrix = csr_matrix((data, (row, col)), shape=(num_samples, num_samples))\n",
    "    sp_affinity_matrix = (sp_affinity_matrix + sp_affinity_matrix.transpose())/2\n",
    "    return sp_affinity_matrix\n",
    "\n",
    "\n",
    "def labelPropagation(sp_affinity, Mat_Label, Mat_Unlabel, labels, alpha=.1, n_iter=100) -> np.array:\n",
    "    '''Propagates the label to get the prefiction for all unlabelled observations\n",
    "    \n",
    "    Args:\n",
    "        sp_affinity: Sparse affinity matrix of shape (num_observation, num_observation).It must have the following structure.\n",
    "        The first 250 rows (assuming our SSL is trained on 250 labels) will be corresponding to 250\n",
    "        labelled examples and remaining rows will be corresponding to unlabelled examples.\n",
    "        \n",
    "        Mat_Label: Feature matrix corresponding to lablled obs.\n",
    "        \n",
    "        Mat_Unlabel: Feature matrix corresponding to unlablled obs.\n",
    "        \n",
    "        labels: labels for each row in Mat_Label\n",
    "    \n",
    "    \n",
    "    Returns:\n",
    "        predicted labels for all rows of Mat_Unlabel\n",
    "    '''\n",
    "    \n",
    "    # initialize\n",
    "    num_label_samples = Mat_Label.shape[0]\n",
    "    num_unlabel_samples = Mat_Unlabel.shape[0]\n",
    "    num_samples = num_label_samples + num_unlabel_samples\n",
    "    labels_list = np.unique(labels)\n",
    "    num_classes = len(labels_list)\n",
    "\n",
    "    clamp_data_label = np.zeros((num_label_samples, num_classes), np.float32)\n",
    "    for i in range(num_label_samples):\n",
    "        clamp_data_label[i][labels[i]] = 1.0\n",
    "\n",
    "    label_function = np.zeros((num_samples, num_classes), np.float32)\n",
    "    label_function[0: num_label_samples] = clamp_data_label\n",
    "    label_function[num_label_samples: num_samples] = 0\n",
    "    \n",
    "    degree_vec = np.sum(sp_affinity, axis=1)\n",
    "    degree_vec_to_the_power_minus_half = 1/np.sqrt(degree_vec)\n",
    "    sp_degree_matrix_2_the_power_minus_half = diags(np.array(degree_vec_to_the_power_minus_half).flatten())\n",
    "\n",
    "    sp_d_minus_half_w_d_minus_half = sp_degree_matrix_2_the_power_minus_half @ sp_affinity @ sp_degree_matrix_2_the_power_minus_half\n",
    "\n",
    "    sparse_matrix = identity(num_samples, format=\"csr\") - alpha * sp_d_minus_half_w_d_minus_half\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    normalization_diag = diags(np.array(1. / degree_vec).flatten())\n",
    "    P = normalization_diag @ sp_affinity\n",
    "    label_function_prop = np.copy(label_function)\n",
    "    for k in range(n_iter):\n",
    "        label_function_prop = P @ label_function_prop\n",
    "        label_function_prop[:num_label_samples] = clamp_data_label\n",
    "        unlabel_data_labels = np.argmax(label_function_prop, axis=1)\n",
    "        unlabel_data_labels = unlabel_data_labels[num_label_samples:]\n",
    "\n",
    "\n",
    "    return unlabel_data_labels, label_function_prop\n",
    "\n",
    "def get_acc(predicted_labels, true_labels):\n",
    "    '''returns accuracy'''\n",
    "    \n",
    "    corrects = 0\n",
    "    num_samples = len(predicted_labels)\n",
    "    for i in range(num_samples):\n",
    "        if predicted_labels[i] == true_labels[i]:\n",
    "            corrects +=1\n",
    "    return corrects/num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "==> Building model..\n"
     ]
    }
   ],
   "source": [
    "print('==> Preparing data..')\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(size=32, scale=(0.2, 1.)),\n",
    "    transforms.ColorJitter(0.4, 0.4, 0.4, 0.4),\n",
    "    transforms.RandomGrayscale(p=0.2),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    Cutout(1, 8),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# labeled dataloader\n",
    "custom_sampler_label = SubsetRandomSampler(label_index)\n",
    "trainset_labeled = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "labeled_dataloader = torch.utils.data.DataLoader(trainset_labeled, batch_size=50, sampler=custom_sampler_label, \n",
    "                                                 num_workers=0)\n",
    "labeled_dataloader_iter = iter(labeled_dataloader)\n",
    "\n",
    "\n",
    "# Unlabeled dataloader\n",
    "custom_sampler_unlabel = SubsetRandomSampler(unlabel_index)\n",
    "\n",
    "\n",
    "trainset = CIFAR10Instance(root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=args.batch_size, sampler=custom_sampler_unlabel, \n",
    "                                          num_workers=20, drop_last=True)\n",
    "testset = CIFAR10Instance(root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=10)\n",
    "\n",
    "\n",
    "print('==> Building model..')\n",
    "net = ResNet18(pool_len=4, low_dim=args.low_dim, fixed_weight=True, temperature=.1)\n",
    "\n",
    "# to use attributes from dataparallel \n",
    "class MyDataParallel(nn.DataParallel):\n",
    "    def __getattr__(self, name):\n",
    "        try:\n",
    "            return super().__getattr__(name)\n",
    "        except AttributeError:\n",
    "            return getattr(self.module, name)\n",
    "\n",
    "if device == 'cuda':\n",
    "    # net = torch.nn.DataParallel(net, device_ids=range(torch.cuda.device_count()))\n",
    "    net = MyDataParallel(net, device_ids=range(torch.cuda.device_count()))\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "\n",
    "# define loss function: inner product loss within each mini-batch\n",
    "criterion = BatchCriterion(args.batch_t, args.batch_size)\n",
    "supervised_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "net.to(device)\n",
    "criterion.to(device)\n",
    "supervised_criterion.to(device)\n",
    "\n",
    "# define optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=args.lr, momentum=0.9, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_finder(nb_epoch, start=-3, end=.1):\n",
    "    \n",
    "    global trainloader, labeled_dataloader, labeled_dataloader_iter\n",
    "    lr_list = np.power(np.repeat(10, len(trainloader)*nb_epoch), np.linspace(start, end, len(trainloader)*nb_epoch))\n",
    "    \n",
    "    net = ResNet18(pool_len=4, low_dim=args.low_dim)\n",
    "    if device == 'cuda':\n",
    "        net = MyDataParallel(net, device_ids=range(torch.cuda.device_count()))\n",
    "        cudnn.benchmark = True\n",
    "    net.to(device)\n",
    "    optimizer = optim.SGD(net.parameters(), lr=args.lr, momentum=0.9, weight_decay=5e-4)\n",
    "    \n",
    "    def adjust_lr_for_lr_finder(optimizer, lr_value):\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr_value\n",
    "\n",
    "    \n",
    "    \n",
    "    current_ind = 0 \n",
    "    def train():\n",
    "        nonlocal current_ind\n",
    "        \n",
    "        train_loss = AverageMeter()\n",
    "        supervised_loss = AverageMeter()\n",
    "        \n",
    "\n",
    "        # switch to train mode\n",
    "        net.train()\n",
    "        \n",
    "        sup_loss_list = []\n",
    "        train_loss_list = []\n",
    "        for batch_idx, (inputs1, inputs2, _, indexes) in enumerate(trainloader):\n",
    "            try:\n",
    "                labeled_input, target = labeled_dataloader_iter.next()\n",
    "            except:\n",
    "                labeled_dataloader_iter = iter(labeled_dataloader)\n",
    "                labeled_input, target = labeled_dataloader_iter.next()\n",
    "\n",
    "            label_batchsize = labeled_input.size(0)\n",
    "            inputs = torch.cat((labeled_input, inputs1, inputs2), 0)\n",
    "            inputs, target, indexes = inputs.to(device), target.to(device), indexes.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            adjust_lr_for_lr_finder(optimizer, lr_list[current_ind])\n",
    "            print(lr_list[current_ind])\n",
    "            current_ind = current_ind + 1\n",
    "\n",
    "            features, pred = net(inputs)\n",
    "            features = features[label_batchsize:]\n",
    "            pred = pred[:label_batchsize]\n",
    "\n",
    "            loss = criterion(features, indexes)\n",
    "            sup_loss = supervised_criterion(pred, target)\n",
    "\n",
    "            factor = 1\n",
    "            final_loss = loss + sup_loss * factor\n",
    "            final_loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            train_loss.update(loss.item(), inputs.size(0))\n",
    "            supervised_loss.update(sup_loss.item(), label_batchsize)\n",
    "            \n",
    "            sup_loss_list.append(supervised_loss.avg)\n",
    "            train_loss_list.append(train_loss.avg)\n",
    "        \n",
    "        return sup_loss_list, train_loss_list\n",
    "    \n",
    "    final_sup_loss = []\n",
    "    final_train_loss = []\n",
    "    for i in range(nb_epoch):\n",
    "        print(f'Epoch: {i}')\n",
    "        sup_loss_list, train_loss_list = train()\n",
    "        final_sup_loss.extend(sup_loss_list)\n",
    "        final_train_loss.extend(train_loss_list)\n",
    "        print(train_loss_list)\n",
    "    \n",
    "    return lr_list, final_train_loss, final_sup_loss\n",
    "\n",
    "# # Uncomment the following to use lr_finder\n",
    "\n",
    "# lr_list, final_train_loss, final_sup_loss = lr_finder(5, end=.5)\n",
    "# plt.plot(lr_list, final_train_loss)\n",
    "# plt.grid(True)\n",
    "# plt.title(\"Learning rate Finder\")\n",
    "# plt.xlabel(\"learning rate\")\n",
    "# plt.ylabel('Loss')\n",
    "# plt.show()\n",
    "# plt.plot(lr_list, final_sup_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_rampdown(current, rampdown_length):\n",
    "    \"\"\"Cosine rampdown from https://arxiv.org/abs/1608.03983\"\"\"\n",
    "    assert 0 <= current <= rampdown_length\n",
    "    return float(.5 * (np.cos(np.pi * current / rampdown_length) + 1))\n",
    "\n",
    "\n",
    "def linear_rampup_new(current, x1, y1, x2, y2):\n",
    "    '''functional value of line between (x1, y1) and (x2, y2)'''\n",
    "    \n",
    "    m = (y1-y2)/(x1-x2)\n",
    "    c = (x1*y2-x2*y1)/(x1-x2)\n",
    "    \n",
    "    return m*current + c\n",
    "\n",
    "def sup_factor_schedule(epoch):\n",
    "    if epoch <= 120:\n",
    "        return .1\n",
    "    else:\n",
    "        return linear_rampup_new(epoch, 120, 0.1, 300, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.stats import bernoulli\n",
    "# beta = np.random.beta(1, 5, 50)\n",
    "# beta = np.maximum(1. - beta, beta)\n",
    "# beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sup_loss_ema = 0 \n",
    "loss_ema = 0\n",
    "def train(epoch, unsupervised=False, sup_factor=.1, ema=False, mixup_aug_unlabel=True, mixup_aug_label = True, mixup_alpha=.1):\n",
    "    global sup_loss_ema, loss_ema\n",
    "    \n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    train_loss = AverageMeter()\n",
    "    supervised_loss = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    batch_time = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    net.train()\n",
    "\n",
    "    end = time.time()\n",
    "    \n",
    "    for batch_idx, (inputs1, inputs2, _, indexes) in enumerate(trainloader):\n",
    "        data_time.update(time.time() - end)\n",
    "#         inputs1, inputs2 = inputs1.to(device), inputs2.to(device)\n",
    "        if unsupervised:\n",
    "            inputs = torch.cat((inputs1, inputs2), 0)\n",
    "            inputs, indexes = inputs.to(device), indexes.to(device)\n",
    "            features, _ = net(inputs)\n",
    "            loss = criterion(features, indexes)\n",
    "            sup_loss = 0\n",
    "            supervised_loss.update(sup_loss, 1) #This step is there so that print doesn't throw any error and I am lazy\n",
    "        else:\n",
    "            \n",
    "            # MIXUP DATA AUGMENTATION for UnLabel\n",
    "            batch_sz = len(inputs1)\n",
    "            if mixup_aug_unlabel:\n",
    "                with torch.no_grad():\n",
    "                    \n",
    "                    #random permutation\n",
    "                    index = np.random.choice(batch_sz,replace=False, size=batch_sz)\n",
    "                    inputs_shuffled = inputs1[index,:,:,:]\n",
    "                    beta_param_1, beta_param_2 = 1, 5 # parameter of the beta distribution\n",
    "                    beta = np.random.beta(beta_param_1, beta_param_2,size=batch_sz)\n",
    "                    beta = np.minimum(1. - beta, beta) #dont want beta to be larger than 0.5\n",
    "                    beta_array = torch.tensor(beta, dtype=torch.float)\n",
    "                    inputs_mixup = (1. - beta_array.view(batch_sz,1,1,1))*inputs1 + \\\n",
    "                                beta_array.view(batch_sz,1,1,1)*inputs_shuffled\n",
    "            \n",
    "            try:\n",
    "                labeled_input, target = labeled_dataloader_iter.next()\n",
    "            except:\n",
    "                labeled_dataloader_iter = iter(labeled_dataloader)\n",
    "                labeled_input, target = labeled_dataloader_iter.next()\n",
    "            \n",
    "#             labeled_input\n",
    "            # MIXUP DATA AUGMENTATION for Label\n",
    "            label_batchsize = labeled_input.size(0)\n",
    "            assert label_batchsize < inputs1.size(0)\n",
    "            if mixup_aug_label:\n",
    "                with torch.no_grad():\n",
    "                    ber = bernoulli.rvs(.5)\n",
    "                    index = np.random.choice(batch_sz,replace=False, size=label_batchsize)\n",
    "                    if ber == 1:\n",
    "                        inputs_shuffled = inputs1[index,:,:,:]\n",
    "                    else:\n",
    "                        inputs_shuffled = inputs2[index,:,:,:]\n",
    "                    beta_param_1, beta_param_2 = 1, 5 # parameter of the beta distribution\n",
    "                    beta = np.random.beta(beta_param_1, beta_param_2,size=label_batchsize)\n",
    "                    beta = np.minimum(1. - beta, beta) #dont want beta to be larger than 0.5\n",
    "                    beta_array = torch.tensor(beta, dtype=torch.float)\n",
    "                    #mixup data-augmentation (keep same label)\n",
    "                    inputs_mixup_label = (1. - beta_array.view(label_batchsize,1,1,1))*labeled_input + \\\n",
    "                                beta_array.view(label_batchsize,1,1,1)*inputs_shuffled\n",
    "                    \n",
    "                    \n",
    "            if mixup_aug_unlabel and mixup_aug_label:\n",
    "                inputs = torch.cat((inputs_mixup_label, inputs_mixup, inputs1, inputs2), 0)\n",
    "                inputs, target, indexes = inputs.to(device), target.to(device), indexes.to(device)\n",
    "                \n",
    "                features, pred = net(inputs)\n",
    "                \n",
    "                pred = pred[:label_batchsize]\n",
    "                \n",
    "                features = features[label_batchsize:]\n",
    "                features_no_mixup = features[batch_sz:]\n",
    "                features_mixup = torch.cat((features[batch_sz:2*batch_sz], features[:batch_sz]), 0)\n",
    "                \n",
    "                loss_no_mixup = criterion(features_no_mixup, indexes)\n",
    "                loss_mixup = criterion(features_mixup, indexes)\n",
    "                loss = (loss_mixup+loss_no_mixup)/2\n",
    "                \n",
    "            else:\n",
    "                if mixup_aug_label:\n",
    "                    inputs = torch.cat((inputs_mixup_label, inputs1, inputs2), 0)\n",
    "                else:\n",
    "                    inputs = torch.cat((labeled_input, inputs1, inputs2), 0)\n",
    "                \n",
    "                inputs, target, indexes = inputs.to(device), target.to(device), indexes.to(device)\n",
    "\n",
    "                features, pred = net(inputs)\n",
    "                features = features[label_batchsize:]\n",
    "                pred = pred[:label_batchsize]\n",
    "\n",
    "                loss = criterion(features, indexes)\n",
    "            \n",
    "            sup_loss = supervised_criterion(pred, target)\n",
    "            supervised_loss.update(sup_loss.item(), label_batchsize)\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "#         sup_factor = linear_rampup(epoch, rampup_length=200)\n",
    "        \n",
    "        if ema:\n",
    "            if epoch == 0 and batch_idx==0:\n",
    "                sup_factor = 1\n",
    "            elif epoch == 0 and batch_idx==1:\n",
    "                sup_loss_ema = sup_loss\n",
    "                loss_ema = loss\n",
    "                sup_factor = loss.item()/sup_loss.item()\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    sup_loss_ema = args.alpha * sup_loss_ema + (1-args.alpha) * sup_loss\n",
    "                    loss_ema = args.alpha * loss_ema + (1-args.alpha) * loss\n",
    "                    sup_factor = loss_ema.item() / sup_loss_ema.item()\n",
    "        \n",
    "        final_loss = loss + sup_loss * sup_factor\n",
    "        final_loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss.update(loss.item(), inputs.size(0))\n",
    "        \n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Epoch: [{}][{}/{}] '\n",
    "                  'Time: {batch_time.val:.3f} ({batch_time.avg:.3f}) '\n",
    "                  'Data: {data_time.val:.3f} ({data_time.avg:.3f}) '\n",
    "                  'Loss: {train_loss.val:.4f} ({train_loss.avg:.4f}) '\n",
    "                  'Sup Loss: {supervised_loss.val:.4f} ({supervised_loss.avg:.4f})'.format(\n",
    "                epoch, batch_idx, len(trainloader), batch_time=batch_time, data_time=data_time, train_loss=train_loss,\n",
    "                supervised_loss=supervised_loss))\n",
    "    # add log\n",
    "    writer.add_scalar('Loss/Unsup', train_loss.avg, epoch)\n",
    "    writer.add_scalar('Loss/Sup', supervised_loss.avg, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Training Model..\n",
      "\n",
      "Epoch: 0\n",
      "Epoch: [0][0/99] Time: 5.835 (5.835) Data: 3.378 (3.378) Loss: 7.4498 (7.4498) Sup Loss: 2.3980 (2.3980)\n",
      "Epoch: [0][10/99] Time: 0.405 (0.947) Data: 0.007 (0.315) Loss: 6.7066 (6.9983) Sup Loss: 2.3418 (2.5476)\n",
      "Epoch: [0][20/99] Time: 0.455 (0.695) Data: 0.040 (0.170) Loss: 6.1306 (6.7115) Sup Loss: 2.1786 (2.4172)\n",
      "Epoch: [0][30/99] Time: 0.400 (0.602) Data: 0.004 (0.117) Loss: 5.6986 (6.4631) Sup Loss: 2.1482 (2.3448)\n",
      "Epoch: [0][40/99] Time: 0.412 (0.553) Data: 0.007 (0.090) Loss: 5.0245 (6.1896) Sup Loss: 2.0314 (2.3018)\n",
      "Epoch: [0][50/99] Time: 0.408 (0.525) Data: 0.006 (0.074) Loss: 4.5244 (5.9149) Sup Loss: 2.0576 (2.2552)\n",
      "Epoch: [0][60/99] Time: 0.419 (0.506) Data: 0.004 (0.063) Loss: 4.2264 (5.6648) Sup Loss: 1.8701 (2.2165)\n",
      "Epoch: [0][70/99] Time: 0.441 (0.493) Data: 0.007 (0.054) Loss: 3.9073 (5.4413) Sup Loss: 1.9367 (2.1783)\n",
      "Epoch: [0][80/99] Time: 0.423 (0.481) Data: 0.004 (0.048) Loss: 3.6984 (5.2445) Sup Loss: 2.0097 (2.1467)\n",
      "Epoch: [0][90/99] Time: 0.410 (0.473) Data: 0.004 (0.043) Loss: 3.5754 (5.0654) Sup Loss: 1.7479 (2.1176)\n",
      "----------Evaluation---------\n",
      "Test [100/10000]\tNet Time 0.026 (0.026)\tCls Time 0.003 (0.003)\tTop1: 52.00  Top5: 94.00\n",
      "Test [200/10000]\tNet Time 0.018 (0.022)\tCls Time 0.001 (0.002)\tTop1: 49.50  Top5: 92.00\n",
      "Test [300/10000]\tNet Time 0.019 (0.021)\tCls Time 0.001 (0.002)\tTop1: 46.67  Top5: 91.33\n",
      "Test [400/10000]\tNet Time 0.020 (0.021)\tCls Time 0.001 (0.001)\tTop1: 48.25  Top5: 91.75\n",
      "Test [500/10000]\tNet Time 0.015 (0.019)\tCls Time 0.001 (0.001)\tTop1: 49.60  Top5: 92.60\n",
      "Test [600/10000]\tNet Time 0.023 (0.020)\tCls Time 0.001 (0.001)\tTop1: 50.50  Top5: 92.83\n",
      "Test [700/10000]\tNet Time 0.022 (0.020)\tCls Time 0.000 (0.001)\tTop1: 49.71  Top5: 92.71\n",
      "Test [800/10000]\tNet Time 0.023 (0.021)\tCls Time 0.001 (0.001)\tTop1: 49.25  Top5: 92.12\n",
      "Test [900/10000]\tNet Time 0.022 (0.021)\tCls Time 0.001 (0.001)\tTop1: 49.89  Top5: 92.67\n",
      "Test [1000/10000]\tNet Time 0.025 (0.021)\tCls Time 0.001 (0.001)\tTop1: 50.40  Top5: 92.80\n",
      "Test [1100/10000]\tNet Time 0.021 (0.021)\tCls Time 0.001 (0.001)\tTop1: 50.64  Top5: 92.45\n",
      "Test [1200/10000]\tNet Time 0.020 (0.021)\tCls Time 0.001 (0.001)\tTop1: 51.17  Top5: 92.42\n",
      "Test [1300/10000]\tNet Time 0.017 (0.021)\tCls Time 0.001 (0.001)\tTop1: 50.62  Top5: 92.54\n",
      "Test [1400/10000]\tNet Time 0.017 (0.021)\tCls Time 0.001 (0.001)\tTop1: 50.29  Top5: 92.36\n",
      "Test [1500/10000]\tNet Time 0.023 (0.021)\tCls Time 0.001 (0.001)\tTop1: 50.13  Top5: 92.40\n",
      "Test [1600/10000]\tNet Time 0.020 (0.021)\tCls Time 0.001 (0.001)\tTop1: 49.38  Top5: 92.31\n",
      "Test [1700/10000]\tNet Time 0.022 (0.021)\tCls Time 0.001 (0.001)\tTop1: 49.47  Top5: 92.12\n",
      "Test [1800/10000]\tNet Time 0.020 (0.021)\tCls Time 0.001 (0.001)\tTop1: 49.50  Top5: 92.17\n",
      "Test [1900/10000]\tNet Time 0.023 (0.021)\tCls Time 0.001 (0.001)\tTop1: 49.74  Top5: 92.26\n",
      "Test [2000/10000]\tNet Time 0.024 (0.021)\tCls Time 0.001 (0.001)\tTop1: 49.30  Top5: 92.15\n",
      "Test [2100/10000]\tNet Time 0.019 (0.021)\tCls Time 0.001 (0.001)\tTop1: 49.38  Top5: 92.29\n",
      "Test [2200/10000]\tNet Time 0.022 (0.021)\tCls Time 0.001 (0.001)\tTop1: 49.41  Top5: 92.23\n",
      "Test [2300/10000]\tNet Time 0.020 (0.021)\tCls Time 0.001 (0.001)\tTop1: 49.43  Top5: 92.22\n",
      "Test [2400/10000]\tNet Time 0.019 (0.021)\tCls Time 0.001 (0.001)\tTop1: 49.58  Top5: 92.21\n",
      "Test [2500/10000]\tNet Time 0.026 (0.021)\tCls Time 0.001 (0.001)\tTop1: 49.64  Top5: 92.12\n",
      "Test [2600/10000]\tNet Time 0.024 (0.021)\tCls Time 0.001 (0.001)\tTop1: 49.23  Top5: 91.92\n",
      "Test [2700/10000]\tNet Time 0.023 (0.021)\tCls Time 0.001 (0.001)\tTop1: 49.07  Top5: 92.11\n",
      "Test [2800/10000]\tNet Time 0.025 (0.021)\tCls Time 0.001 (0.001)\tTop1: 49.04  Top5: 92.11\n",
      "Test [2900/10000]\tNet Time 0.023 (0.021)\tCls Time 0.001 (0.001)\tTop1: 49.00  Top5: 92.14\n",
      "Test [3000/10000]\tNet Time 0.025 (0.021)\tCls Time 0.001 (0.001)\tTop1: 49.03  Top5: 92.13\n",
      "Test [3100/10000]\tNet Time 0.019 (0.021)\tCls Time 0.001 (0.001)\tTop1: 49.00  Top5: 92.23\n",
      "Test [3200/10000]\tNet Time 0.023 (0.021)\tCls Time 0.001 (0.001)\tTop1: 49.16  Top5: 92.38\n",
      "Test [3300/10000]\tNet Time 0.022 (0.021)\tCls Time 0.001 (0.001)\tTop1: 49.00  Top5: 92.27\n",
      "Test [3400/10000]\tNet Time 0.019 (0.021)\tCls Time 0.001 (0.001)\tTop1: 48.85  Top5: 92.21\n",
      "Test [3500/10000]\tNet Time 0.023 (0.021)\tCls Time 0.001 (0.001)\tTop1: 48.83  Top5: 92.06\n",
      "Test [3600/10000]\tNet Time 0.018 (0.021)\tCls Time 0.001 (0.001)\tTop1: 49.03  Top5: 92.06\n",
      "Test [3700/10000]\tNet Time 0.024 (0.021)\tCls Time 0.001 (0.001)\tTop1: 49.03  Top5: 92.08\n",
      "Test [3800/10000]\tNet Time 0.024 (0.021)\tCls Time 0.001 (0.001)\tTop1: 48.92  Top5: 92.08\n",
      "Test [3900/10000]\tNet Time 0.023 (0.022)\tCls Time 0.001 (0.001)\tTop1: 49.00  Top5: 92.15\n",
      "Test [4000/10000]\tNet Time 0.020 (0.021)\tCls Time 0.001 (0.001)\tTop1: 48.70  Top5: 92.20\n",
      "Test [4100/10000]\tNet Time 0.020 (0.021)\tCls Time 0.001 (0.001)\tTop1: 48.68  Top5: 92.22\n",
      "Test [4200/10000]\tNet Time 0.022 (0.021)\tCls Time 0.001 (0.001)\tTop1: 48.74  Top5: 92.19\n",
      "Test [4300/10000]\tNet Time 0.020 (0.021)\tCls Time 0.001 (0.001)\tTop1: 48.79  Top5: 92.28\n",
      "Test [4400/10000]\tNet Time 0.023 (0.021)\tCls Time 0.001 (0.001)\tTop1: 49.05  Top5: 92.34\n",
      "Test [4500/10000]\tNet Time 0.024 (0.022)\tCls Time 0.001 (0.001)\tTop1: 49.02  Top5: 92.38\n",
      "Test [4600/10000]\tNet Time 0.018 (0.021)\tCls Time 0.001 (0.001)\tTop1: 48.83  Top5: 92.39\n",
      "Test [4700/10000]\tNet Time 0.026 (0.022)\tCls Time 0.001 (0.001)\tTop1: 48.83  Top5: 92.43\n",
      "Test [4800/10000]\tNet Time 0.024 (0.022)\tCls Time 0.001 (0.001)\tTop1: 48.92  Top5: 92.35\n",
      "Test [4900/10000]\tNet Time 0.023 (0.022)\tCls Time 0.001 (0.001)\tTop1: 48.90  Top5: 92.20\n",
      "Test [5000/10000]\tNet Time 0.025 (0.022)\tCls Time 0.001 (0.001)\tTop1: 48.86  Top5: 92.26\n",
      "Test [5100/10000]\tNet Time 0.023 (0.022)\tCls Time 0.001 (0.001)\tTop1: 48.96  Top5: 92.22\n",
      "Test [5200/10000]\tNet Time 0.021 (0.022)\tCls Time 0.001 (0.001)\tTop1: 48.85  Top5: 92.17\n",
      "Test [5300/10000]\tNet Time 0.023 (0.022)\tCls Time 0.001 (0.001)\tTop1: 48.79  Top5: 92.09\n",
      "Test [5400/10000]\tNet Time 0.023 (0.022)\tCls Time 0.001 (0.001)\tTop1: 48.70  Top5: 91.98\n",
      "Test [5500/10000]\tNet Time 0.022 (0.022)\tCls Time 0.001 (0.001)\tTop1: 48.67  Top5: 92.02\n",
      "Test [5600/10000]\tNet Time 0.023 (0.022)\tCls Time 0.001 (0.001)\tTop1: 48.68  Top5: 91.96\n",
      "Test [5700/10000]\tNet Time 0.023 (0.022)\tCls Time 0.001 (0.001)\tTop1: 48.60  Top5: 91.91\n",
      "Test [5800/10000]\tNet Time 0.022 (0.022)\tCls Time 0.001 (0.001)\tTop1: 48.71  Top5: 91.98\n",
      "Test [5900/10000]\tNet Time 0.022 (0.022)\tCls Time 0.001 (0.001)\tTop1: 48.61  Top5: 92.00\n",
      "Test [6000/10000]\tNet Time 0.024 (0.022)\tCls Time 0.001 (0.001)\tTop1: 48.57  Top5: 92.03\n",
      "Test [6100/10000]\tNet Time 0.022 (0.022)\tCls Time 0.001 (0.001)\tTop1: 48.52  Top5: 91.98\n",
      "Test [6200/10000]\tNet Time 0.024 (0.022)\tCls Time 0.001 (0.001)\tTop1: 48.50  Top5: 91.97\n",
      "Test [6300/10000]\tNet Time 0.024 (0.022)\tCls Time 0.001 (0.001)\tTop1: 48.48  Top5: 92.02\n",
      "Test [6400/10000]\tNet Time 0.026 (0.022)\tCls Time 0.001 (0.001)\tTop1: 48.50  Top5: 92.00\n",
      "Test [6500/10000]\tNet Time 0.027 (0.022)\tCls Time 0.001 (0.001)\tTop1: 48.48  Top5: 91.98\n",
      "Test [6600/10000]\tNet Time 0.023 (0.022)\tCls Time 0.001 (0.001)\tTop1: 48.42  Top5: 92.00\n",
      "Test [6700/10000]\tNet Time 0.022 (0.022)\tCls Time 0.001 (0.001)\tTop1: 48.51  Top5: 91.97\n",
      "Test [6800/10000]\tNet Time 0.021 (0.022)\tCls Time 0.001 (0.001)\tTop1: 48.50  Top5: 91.96\n",
      "Test [6900/10000]\tNet Time 0.022 (0.022)\tCls Time 0.001 (0.001)\tTop1: 48.51  Top5: 91.90\n",
      "Test [7000/10000]\tNet Time 0.026 (0.022)\tCls Time 0.001 (0.001)\tTop1: 48.50  Top5: 91.84\n",
      "Test [7100/10000]\tNet Time 0.027 (0.022)\tCls Time 0.001 (0.001)\tTop1: 48.49  Top5: 91.82\n",
      "Test [7200/10000]\tNet Time 0.025 (0.022)\tCls Time 0.001 (0.001)\tTop1: 48.46  Top5: 91.86\n",
      "Test [7300/10000]\tNet Time 0.024 (0.022)\tCls Time 0.001 (0.001)\tTop1: 48.42  Top5: 91.88\n",
      "Test [7400/10000]\tNet Time 0.026 (0.022)\tCls Time 0.001 (0.001)\tTop1: 48.58  Top5: 91.93\n",
      "Test [7500/10000]\tNet Time 0.025 (0.022)\tCls Time 0.001 (0.001)\tTop1: 48.52  Top5: 91.92\n",
      "Test [7600/10000]\tNet Time 0.024 (0.022)\tCls Time 0.001 (0.001)\tTop1: 48.43  Top5: 91.92\n",
      "Test [7700/10000]\tNet Time 0.025 (0.022)\tCls Time 0.001 (0.001)\tTop1: 48.40  Top5: 91.91\n",
      "Test [7800/10000]\tNet Time 0.024 (0.022)\tCls Time 0.001 (0.001)\tTop1: 48.33  Top5: 91.90\n",
      "Test [7900/10000]\tNet Time 0.025 (0.022)\tCls Time 0.001 (0.001)\tTop1: 48.27  Top5: 91.89\n",
      "Test [8000/10000]\tNet Time 0.025 (0.022)\tCls Time 0.001 (0.001)\tTop1: 48.17  Top5: 91.83\n",
      "Test [8100/10000]\tNet Time 0.025 (0.023)\tCls Time 0.001 (0.001)\tTop1: 48.12  Top5: 91.79\n",
      "Test [8200/10000]\tNet Time 0.027 (0.023)\tCls Time 0.001 (0.001)\tTop1: 48.05  Top5: 91.76\n",
      "Test [8300/10000]\tNet Time 0.024 (0.023)\tCls Time 0.001 (0.001)\tTop1: 47.98  Top5: 91.76\n",
      "Test [8400/10000]\tNet Time 0.024 (0.023)\tCls Time 0.001 (0.001)\tTop1: 47.92  Top5: 91.74\n",
      "Test [8500/10000]\tNet Time 0.023 (0.023)\tCls Time 0.001 (0.001)\tTop1: 47.84  Top5: 91.71\n",
      "Test [8600/10000]\tNet Time 0.024 (0.023)\tCls Time 0.001 (0.001)\tTop1: 47.88  Top5: 91.73\n",
      "Test [8700/10000]\tNet Time 0.023 (0.023)\tCls Time 0.001 (0.001)\tTop1: 47.89  Top5: 91.74\n",
      "Test [8800/10000]\tNet Time 0.025 (0.023)\tCls Time 0.001 (0.001)\tTop1: 47.91  Top5: 91.76\n",
      "Test [8900/10000]\tNet Time 0.024 (0.023)\tCls Time 0.001 (0.001)\tTop1: 47.99  Top5: 91.73\n",
      "Test [9000/10000]\tNet Time 0.024 (0.023)\tCls Time 0.001 (0.001)\tTop1: 48.03  Top5: 91.72\n",
      "Test [9100/10000]\tNet Time 0.023 (0.023)\tCls Time 0.001 (0.001)\tTop1: 48.02  Top5: 91.73\n",
      "Test [9200/10000]\tNet Time 0.025 (0.023)\tCls Time 0.001 (0.001)\tTop1: 48.20  Top5: 91.76\n",
      "Test [9300/10000]\tNet Time 0.025 (0.023)\tCls Time 0.001 (0.001)\tTop1: 48.33  Top5: 91.82\n",
      "Test [9400/10000]\tNet Time 0.023 (0.023)\tCls Time 0.001 (0.001)\tTop1: 48.37  Top5: 91.80\n",
      "Test [9500/10000]\tNet Time 0.033 (0.023)\tCls Time 0.001 (0.001)\tTop1: 48.36  Top5: 91.81\n",
      "Test [9600/10000]\tNet Time 0.026 (0.023)\tCls Time 0.001 (0.001)\tTop1: 48.35  Top5: 91.82\n",
      "Test [9700/10000]\tNet Time 0.025 (0.023)\tCls Time 0.001 (0.001)\tTop1: 48.38  Top5: 91.82\n",
      "Test [9800/10000]\tNet Time 0.024 (0.023)\tCls Time 0.001 (0.001)\tTop1: 48.33  Top5: 91.85\n",
      "Test [9900/10000]\tNet Time 0.025 (0.023)\tCls Time 0.001 (0.001)\tTop1: 48.30  Top5: 91.85\n",
      "Test [10000/10000]\tNet Time 0.025 (0.023)\tCls Time 0.001 (0.001)\tTop1: 48.29  Top5: 91.86\n",
      "48.29\n",
      "Evaluation Time:18.66\n",
      "Saving..\n",
      "accuracy: 48.3% \t (best acc: 48.3%)\n",
      "\n",
      "Epoch: 1\n",
      "Epoch: [1][0/99] Time: 4.164 (4.164) Data: 3.551 (3.551) Loss: 3.4512 (3.4512) Sup Loss: 1.4394 (1.4394)\n",
      "Epoch: [1][10/99] Time: 0.435 (0.805) Data: 0.017 (0.340) Loss: 3.3508 (3.3661) Sup Loss: 1.7313 (1.7191)\n",
      "Epoch: [1][20/99] Time: 0.448 (0.624) Data: 0.010 (0.181) Loss: 3.1835 (3.3041) Sup Loss: 1.6280 (1.6825)\n",
      "Epoch: [1][30/99] Time: 0.443 (0.564) Data: 0.008 (0.127) Loss: 3.1585 (3.2528) Sup Loss: 1.5534 (1.6455)\n",
      "Epoch: [1][40/99] Time: 0.452 (0.530) Data: 0.009 (0.097) Loss: 3.1092 (3.2042) Sup Loss: 1.4140 (1.6161)\n",
      "Epoch: [1][50/99] Time: 0.429 (0.508) Data: 0.009 (0.080) Loss: 3.0407 (3.1658) Sup Loss: 1.3778 (1.5943)\n",
      "Epoch: [1][60/99] Time: 0.427 (0.496) Data: 0.003 (0.068) Loss: 2.9777 (3.1316) Sup Loss: 1.3123 (1.5758)\n",
      "Epoch: [1][70/99] Time: 0.388 (0.484) Data: 0.004 (0.059) Loss: 2.7699 (3.0970) Sup Loss: 1.4756 (1.5650)\n",
      "Epoch: [1][80/99] Time: 0.402 (0.475) Data: 0.004 (0.052) Loss: 2.8292 (3.0635) Sup Loss: 1.2839 (1.5362)\n",
      "Epoch: [1][90/99] Time: 0.432 (0.468) Data: 0.004 (0.047) Loss: 2.5993 (3.0262) Sup Loss: 1.4449 (1.5169)\n",
      "\n",
      "Epoch: 2\n",
      "Epoch: [2][0/99] Time: 4.184 (4.184) Data: 3.652 (3.652) Loss: 2.6193 (2.6193) Sup Loss: 1.2396 (1.2396)\n",
      "Epoch: [2][10/99] Time: 0.425 (0.792) Data: 0.006 (0.344) Loss: 2.6117 (2.6611) Sup Loss: 1.0291 (1.2184)\n",
      "Epoch: [2][20/99] Time: 0.434 (0.617) Data: 0.007 (0.183) Loss: 2.5798 (2.6413) Sup Loss: 0.9252 (1.2177)\n",
      "Epoch: [2][30/99] Time: 0.463 (0.559) Data: 0.006 (0.127) Loss: 2.5942 (2.6179) Sup Loss: 1.0697 (1.1990)\n",
      "Epoch: [2][40/99] Time: 0.454 (0.526) Data: 0.010 (0.098) Loss: 2.4821 (2.6024) Sup Loss: 1.1431 (1.1828)\n",
      "Epoch: [2][50/99] Time: 0.464 (0.506) Data: 0.006 (0.080) Loss: 2.5027 (2.5879) Sup Loss: 0.8959 (1.1668)\n",
      "Epoch: [2][60/99] Time: 0.413 (0.493) Data: 0.004 (0.068) Loss: 2.5827 (2.5712) Sup Loss: 0.9973 (1.1551)\n",
      "Epoch: [2][70/99] Time: 0.423 (0.480) Data: 0.006 (0.059) Loss: 2.4690 (2.5569) Sup Loss: 0.8628 (1.1336)\n",
      "Epoch: [2][80/99] Time: 0.402 (0.472) Data: 0.003 (0.052) Loss: 2.4378 (2.5353) Sup Loss: 1.0966 (1.1164)\n",
      "Epoch: [2][90/99] Time: 0.448 (0.465) Data: 0.006 (0.047) Loss: 2.3164 (2.5191) Sup Loss: 0.7972 (1.0989)\n",
      "\n",
      "Epoch: 3\n",
      "Epoch: [3][0/99] Time: 4.425 (4.425) Data: 3.882 (3.882) Loss: 2.3819 (2.3819) Sup Loss: 0.8405 (0.8405)\n",
      "Epoch: [3][10/99] Time: 0.418 (0.813) Data: 0.007 (0.365) Loss: 2.3447 (2.3593) Sup Loss: 1.0259 (0.9206)\n",
      "Epoch: [3][20/99] Time: 0.465 (0.628) Data: 0.026 (0.195) Loss: 2.3409 (2.3403) Sup Loss: 0.8013 (0.8822)\n",
      "Epoch: [3][30/99] Time: 0.394 (0.561) Data: 0.004 (0.134) Loss: 2.3808 (2.3401) Sup Loss: 1.0125 (0.8645)\n",
      "Epoch: [3][40/99] Time: 0.441 (0.529) Data: 0.008 (0.103) Loss: 2.2137 (2.3325) Sup Loss: 0.9834 (0.8822)\n",
      "Epoch: [3][50/99] Time: 0.401 (0.510) Data: 0.008 (0.084) Loss: 2.2898 (2.3212) Sup Loss: 0.7024 (0.8459)\n",
      "Epoch: [3][60/99] Time: 0.427 (0.496) Data: 0.005 (0.072) Loss: 2.2496 (2.3137) Sup Loss: 1.0071 (0.8454)\n",
      "Epoch: [3][70/99] Time: 0.403 (0.485) Data: 0.007 (0.062) Loss: 2.3013 (2.2997) Sup Loss: 0.8239 (0.8339)\n",
      "Epoch: [3][80/99] Time: 0.386 (0.476) Data: 0.003 (0.055) Loss: 2.2279 (2.2895) Sup Loss: 0.8237 (0.8276)\n",
      "Epoch: [3][90/99] Time: 0.433 (0.469) Data: 0.008 (0.050) Loss: 2.1984 (2.2791) Sup Loss: 0.7743 (0.8287)\n",
      "\n",
      "Epoch: 4\n",
      "Epoch: [4][0/99] Time: 4.516 (4.516) Data: 3.887 (3.887) Loss: 2.1799 (2.1799) Sup Loss: 0.7270 (0.7270)\n",
      "Epoch: [4][10/99] Time: 0.422 (0.815) Data: 0.005 (0.360) Loss: 2.2071 (2.1697) Sup Loss: 0.6016 (0.6762)\n",
      "Epoch: [4][20/99] Time: 0.463 (0.639) Data: 0.044 (0.194) Loss: 2.1282 (2.1687) Sup Loss: 0.8332 (0.7323)\n",
      "Epoch: [4][30/99] Time: 0.471 (0.570) Data: 0.007 (0.133) Loss: 2.0761 (2.1536) Sup Loss: 0.8175 (0.7487)\n",
      "Epoch: [4][40/99] Time: 0.429 (0.534) Data: 0.014 (0.102) Loss: 2.1670 (2.1476) Sup Loss: 0.8060 (0.7368)\n",
      "Epoch: [4][50/99] Time: 0.447 (0.512) Data: 0.012 (0.083) Loss: 2.0877 (2.1419) Sup Loss: 0.6789 (0.7387)\n",
      "Epoch: [4][60/99] Time: 0.414 (0.497) Data: 0.005 (0.071) Loss: 2.0529 (2.1307) Sup Loss: 0.6636 (0.7328)\n",
      "Epoch: [4][70/99] Time: 0.417 (0.486) Data: 0.004 (0.062) Loss: 2.0517 (2.1227) Sup Loss: 0.5367 (0.7248)\n",
      "Epoch: [4][80/99] Time: 0.409 (0.478) Data: 0.004 (0.055) Loss: 2.0431 (2.1135) Sup Loss: 0.5325 (0.7211)\n",
      "Epoch: [4][90/99] Time: 0.429 (0.470) Data: 0.004 (0.049) Loss: 2.0538 (2.1089) Sup Loss: 0.6111 (0.7159)\n",
      "\n",
      "Epoch: 5\n",
      "Epoch: [5][0/99] Time: 4.525 (4.525) Data: 3.901 (3.901) Loss: 1.9292 (1.9292) Sup Loss: 0.7759 (0.7759)\n",
      "Epoch: [5][10/99] Time: 0.441 (0.822) Data: 0.013 (0.364) Loss: 1.9764 (2.0320) Sup Loss: 0.7030 (0.7132)\n",
      "Epoch: [5][20/99] Time: 0.470 (0.638) Data: 0.029 (0.194) Loss: 1.9498 (2.0270) Sup Loss: 0.5973 (0.6929)\n",
      "Epoch: [5][30/99] Time: 0.433 (0.570) Data: 0.008 (0.134) Loss: 1.9565 (2.0269) Sup Loss: 0.6804 (0.6635)\n",
      "Epoch: [5][40/99] Time: 0.399 (0.535) Data: 0.009 (0.103) Loss: 2.0351 (2.0217) Sup Loss: 0.7368 (0.6478)\n",
      "Epoch: [5][50/99] Time: 0.410 (0.513) Data: 0.007 (0.084) Loss: 2.0186 (2.0145) Sup Loss: 0.6539 (0.6472)\n",
      "Epoch: [5][60/99] Time: 0.409 (0.499) Data: 0.006 (0.071) Loss: 1.9527 (2.0106) Sup Loss: 0.6084 (0.6348)\n",
      "Epoch: [5][70/99] Time: 0.418 (0.488) Data: 0.005 (0.062) Loss: 1.9844 (2.0055) Sup Loss: 0.4863 (0.6283)\n",
      "Epoch: [5][80/99] Time: 0.424 (0.478) Data: 0.004 (0.055) Loss: 1.9387 (1.9965) Sup Loss: 0.4718 (0.6216)\n",
      "Epoch: [5][90/99] Time: 0.405 (0.470) Data: 0.003 (0.049) Loss: 1.8604 (1.9946) Sup Loss: 0.6605 (0.6158)\n",
      "\n",
      "Epoch: 6\n",
      "Epoch: [6][0/99] Time: 4.324 (4.324) Data: 3.715 (3.715) Loss: 1.9259 (1.9259) Sup Loss: 0.4854 (0.4854)\n",
      "Epoch: [6][10/99] Time: 0.443 (0.815) Data: 0.005 (0.350) Loss: 1.9203 (1.9259) Sup Loss: 0.6504 (0.6157)\n",
      "Epoch: [6][20/99] Time: 0.470 (0.631) Data: 0.008 (0.186) Loss: 1.9528 (1.9216) Sup Loss: 0.4779 (0.5888)\n",
      "Epoch: [6][30/99] Time: 0.422 (0.565) Data: 0.005 (0.129) Loss: 1.9129 (1.9151) Sup Loss: 0.4919 (0.5843)\n",
      "Epoch: [6][40/99] Time: 0.429 (0.531) Data: 0.009 (0.099) Loss: 1.8651 (1.9136) Sup Loss: 0.5060 (0.5922)\n",
      "Epoch: [6][50/99] Time: 0.410 (0.510) Data: 0.006 (0.081) Loss: 1.9680 (1.9149) Sup Loss: 0.5026 (0.5890)\n",
      "Epoch: [6][60/99] Time: 0.435 (0.498) Data: 0.004 (0.069) Loss: 1.8363 (1.9139) Sup Loss: 0.5810 (0.5840)\n",
      "Epoch: [6][70/99] Time: 0.422 (0.487) Data: 0.004 (0.060) Loss: 1.8853 (1.9110) Sup Loss: 0.6243 (0.5829)\n",
      "Epoch: [6][80/99] Time: 0.444 (0.478) Data: 0.005 (0.053) Loss: 1.8951 (1.9039) Sup Loss: 0.5576 (0.5691)\n",
      "Epoch: [6][90/99] Time: 0.432 (0.471) Data: 0.006 (0.048) Loss: 1.8697 (1.9005) Sup Loss: 0.3523 (0.5671)\n",
      "\n",
      "Epoch: 7\n",
      "Epoch: [7][0/99] Time: 4.210 (4.210) Data: 3.669 (3.669) Loss: 1.8815 (1.8815) Sup Loss: 0.4980 (0.4980)\n",
      "Epoch: [7][10/99] Time: 0.418 (0.793) Data: 0.007 (0.346) Loss: 1.9167 (1.8628) Sup Loss: 0.4997 (0.5279)\n",
      "Epoch: [7][20/99] Time: 0.418 (0.615) Data: 0.007 (0.184) Loss: 1.8305 (1.8605) Sup Loss: 0.4276 (0.5496)\n",
      "Epoch: [7][30/99] Time: 0.420 (0.554) Data: 0.004 (0.127) Loss: 1.7891 (1.8525) Sup Loss: 0.6423 (0.5365)\n",
      "Epoch: [7][40/99] Time: 0.427 (0.523) Data: 0.008 (0.098) Loss: 1.7749 (1.8420) Sup Loss: 0.5868 (0.5260)\n",
      "Epoch: [7][50/99] Time: 0.451 (0.505) Data: 0.007 (0.080) Loss: 1.8204 (1.8409) Sup Loss: 0.4057 (0.5189)\n",
      "Epoch: [7][60/99] Time: 0.409 (0.491) Data: 0.004 (0.068) Loss: 1.8409 (1.8391) Sup Loss: 0.4796 (0.5206)\n",
      "Epoch: [7][70/99] Time: 0.427 (0.481) Data: 0.007 (0.059) Loss: 1.8853 (1.8376) Sup Loss: 0.8950 (0.5253)\n",
      "Epoch: [7][80/99] Time: 0.392 (0.473) Data: 0.003 (0.052) Loss: 1.8697 (1.8342) Sup Loss: 0.6092 (0.5208)\n",
      "Epoch: [7][90/99] Time: 0.403 (0.465) Data: 0.004 (0.047) Loss: 1.8312 (1.8340) Sup Loss: 0.4588 (0.5195)\n",
      "\n",
      "Epoch: 8\n",
      "Epoch: [8][0/99] Time: 4.388 (4.388) Data: 3.875 (3.875) Loss: 1.7627 (1.7627) Sup Loss: 0.5724 (0.5724)\n",
      "Epoch: [8][10/99] Time: 0.447 (0.811) Data: 0.004 (0.366) Loss: 1.7341 (1.7758) Sup Loss: 0.4661 (0.5032)\n",
      "Epoch: [8][20/99] Time: 0.438 (0.627) Data: 0.024 (0.195) Loss: 1.8335 (1.7929) Sup Loss: 0.5078 (0.4916)\n",
      "Epoch: [8][30/99] Time: 0.426 (0.566) Data: 0.004 (0.135) Loss: 1.8665 (1.7983) Sup Loss: 0.5554 (0.4709)\n",
      "Epoch: [8][40/99] Time: 0.450 (0.534) Data: 0.013 (0.104) Loss: 1.8176 (1.7959) Sup Loss: 0.5088 (0.4767)\n",
      "Epoch: [8][50/99] Time: 0.446 (0.513) Data: 0.009 (0.085) Loss: 1.7800 (1.7918) Sup Loss: 0.5428 (0.4844)\n",
      "Epoch: [8][60/99] Time: 0.427 (0.499) Data: 0.004 (0.072) Loss: 1.7593 (1.7886) Sup Loss: 0.5925 (0.4824)\n",
      "Epoch: [8][70/99] Time: 0.424 (0.488) Data: 0.004 (0.062) Loss: 1.7363 (1.7866) Sup Loss: 0.5816 (0.4835)\n",
      "Epoch: [8][80/99] Time: 0.427 (0.479) Data: 0.003 (0.055) Loss: 1.7869 (1.7837) Sup Loss: 0.3495 (0.4833)\n",
      "Epoch: [8][90/99] Time: 0.430 (0.471) Data: 0.004 (0.050) Loss: 1.7229 (1.7811) Sup Loss: 0.4744 (0.4920)\n",
      "\n",
      "Epoch: 9\n",
      "Epoch: [9][0/99] Time: 4.398 (4.398) Data: 3.759 (3.759) Loss: 1.8295 (1.8295) Sup Loss: 0.3623 (0.3623)\n",
      "Epoch: [9][10/99] Time: 0.400 (0.810) Data: 0.005 (0.355) Loss: 1.7335 (1.7461) Sup Loss: 0.3910 (0.3938)\n",
      "Epoch: [9][20/99] Time: 0.398 (0.628) Data: 0.008 (0.189) Loss: 1.6786 (1.7514) Sup Loss: 0.2932 (0.4355)\n",
      "Epoch: [9][30/99] Time: 0.440 (0.563) Data: 0.004 (0.131) Loss: 1.7385 (1.7426) Sup Loss: 0.4063 (0.4532)\n",
      "Epoch: [9][40/99] Time: 0.422 (0.532) Data: 0.006 (0.100) Loss: 1.6940 (1.7405) Sup Loss: 0.3823 (0.4497)\n",
      "Epoch: [9][50/99] Time: 0.440 (0.510) Data: 0.010 (0.082) Loss: 1.7021 (1.7379) Sup Loss: 0.3394 (0.4476)\n",
      "Epoch: [9][60/99] Time: 0.405 (0.496) Data: 0.005 (0.070) Loss: 1.6761 (1.7350) Sup Loss: 0.3754 (0.4456)\n",
      "Epoch: [9][70/99] Time: 0.406 (0.484) Data: 0.006 (0.061) Loss: 1.6658 (1.7303) Sup Loss: 0.5188 (0.4477)\n",
      "Epoch: [9][80/99] Time: 0.417 (0.476) Data: 0.004 (0.054) Loss: 1.6755 (1.7282) Sup Loss: 0.5158 (0.4517)\n",
      "Epoch: [9][90/99] Time: 0.412 (0.469) Data: 0.004 (0.048) Loss: 1.6564 (1.7264) Sup Loss: 0.4194 (0.4502)\n",
      "\n",
      "Epoch: 10\n",
      "Epoch: [10][0/99] Time: 4.304 (4.304) Data: 3.782 (3.782) Loss: 1.6535 (1.6535) Sup Loss: 0.4351 (0.4351)\n",
      "Epoch: [10][10/99] Time: 0.464 (0.801) Data: 0.013 (0.356) Loss: 1.6395 (1.6744) Sup Loss: 0.5049 (0.4118)\n",
      "Epoch: [10][20/99] Time: 0.465 (0.628) Data: 0.031 (0.190) Loss: 1.6334 (1.7011) Sup Loss: 0.3535 (0.4233)\n",
      "Epoch: [10][30/99] Time: 0.418 (0.563) Data: 0.014 (0.131) Loss: 1.6423 (1.6935) Sup Loss: 0.4764 (0.4301)\n",
      "Epoch: [10][40/99] Time: 0.421 (0.531) Data: 0.011 (0.101) Loss: 1.7225 (1.6955) Sup Loss: 0.4994 (0.4346)\n",
      "Epoch: [10][50/99] Time: 0.440 (0.508) Data: 0.006 (0.082) Loss: 1.7656 (1.6948) Sup Loss: 0.5242 (0.4372)\n",
      "Epoch: [10][60/99] Time: 0.430 (0.493) Data: 0.004 (0.070) Loss: 1.7046 (1.6952) Sup Loss: 0.3477 (0.4339)\n",
      "Epoch: [10][70/99] Time: 0.410 (0.481) Data: 0.004 (0.061) Loss: 1.7455 (1.6898) Sup Loss: 0.6699 (0.4352)\n",
      "Epoch: [10][80/99] Time: 0.412 (0.473) Data: 0.004 (0.054) Loss: 1.7094 (1.6890) Sup Loss: 0.3556 (0.4348)\n",
      "Epoch: [10][90/99] Time: 0.439 (0.466) Data: 0.005 (0.048) Loss: 1.6294 (1.6870) Sup Loss: 0.5122 (0.4341)\n",
      "\n",
      "Epoch: 11\n",
      "Epoch: [11][0/99] Time: 4.476 (4.476) Data: 3.824 (3.824) Loss: 1.6169 (1.6169) Sup Loss: 0.4378 (0.4378)\n",
      "Epoch: [11][10/99] Time: 0.411 (0.814) Data: 0.007 (0.357) Loss: 1.6653 (1.6650) Sup Loss: 0.2179 (0.4184)\n",
      "Epoch: [11][20/99] Time: 0.426 (0.628) Data: 0.013 (0.190) Loss: 1.7010 (1.6703) Sup Loss: 0.3423 (0.4241)\n",
      "Epoch: [11][30/99] Time: 0.444 (0.564) Data: 0.006 (0.132) Loss: 1.5957 (1.6690) Sup Loss: 0.3680 (0.4112)\n",
      "Epoch: [11][40/99] Time: 0.411 (0.530) Data: 0.008 (0.101) Loss: 1.7811 (1.6663) Sup Loss: 0.3785 (0.4101)\n",
      "Epoch: [11][50/99] Time: 0.422 (0.511) Data: 0.007 (0.083) Loss: 1.7303 (1.6674) Sup Loss: 0.5401 (0.4148)\n",
      "Epoch: [11][60/99] Time: 0.395 (0.495) Data: 0.004 (0.070) Loss: 1.6292 (1.6638) Sup Loss: 0.5187 (0.4212)\n",
      "Epoch: [11][70/99] Time: 0.401 (0.484) Data: 0.004 (0.061) Loss: 1.6991 (1.6636) Sup Loss: 0.2716 (0.4218)\n",
      "Epoch: [11][80/99] Time: 0.433 (0.476) Data: 0.004 (0.054) Loss: 1.6558 (1.6626) Sup Loss: 0.4097 (0.4214)\n",
      "Epoch: [11][90/99] Time: 0.388 (0.470) Data: 0.004 (0.049) Loss: 1.6984 (1.6594) Sup Loss: 0.5400 (0.4184)\n",
      "\n",
      "Epoch: 12\n",
      "Epoch: [12][0/99] Time: 4.794 (4.794) Data: 4.202 (4.202) Loss: 1.6108 (1.6108) Sup Loss: 0.2763 (0.2763)\n",
      "Epoch: [12][10/99] Time: 0.410 (0.839) Data: 0.004 (0.387) Loss: 1.6766 (1.6347) Sup Loss: 0.4208 (0.3577)\n",
      "Epoch: [12][20/99] Time: 0.469 (0.645) Data: 0.042 (0.208) Loss: 1.6352 (1.6296) Sup Loss: 0.4125 (0.3699)\n",
      "Epoch: [12][30/99] Time: 0.454 (0.571) Data: 0.005 (0.142) Loss: 1.6387 (1.6168) Sup Loss: 0.5065 (0.3942)\n",
      "Epoch: [12][40/99] Time: 0.447 (0.537) Data: 0.009 (0.109) Loss: 1.6000 (1.6248) Sup Loss: 0.4577 (0.4078)\n",
      "Epoch: [12][50/99] Time: 0.424 (0.515) Data: 0.011 (0.089) Loss: 1.6376 (1.6248) Sup Loss: 0.4924 (0.4107)\n",
      "Epoch: [12][60/99] Time: 0.404 (0.499) Data: 0.004 (0.076) Loss: 1.6793 (1.6274) Sup Loss: 0.4448 (0.4059)\n",
      "Epoch: [12][70/99] Time: 0.432 (0.487) Data: 0.004 (0.066) Loss: 1.6014 (1.6243) Sup Loss: 0.3665 (0.4060)\n",
      "Epoch: [12][80/99] Time: 0.409 (0.480) Data: 0.004 (0.058) Loss: 1.5849 (1.6237) Sup Loss: 0.4517 (0.4037)\n",
      "Epoch: [12][90/99] Time: 0.429 (0.472) Data: 0.004 (0.053) Loss: 1.6355 (1.6231) Sup Loss: 0.5337 (0.4074)\n",
      "\n",
      "Epoch: 13\n",
      "Epoch: [13][0/99] Time: 4.299 (4.299) Data: 3.734 (3.734) Loss: 1.6041 (1.6041) Sup Loss: 0.3353 (0.3353)\n",
      "Epoch: [13][10/99] Time: 0.397 (0.793) Data: 0.004 (0.349) Loss: 1.6112 (1.5936) Sup Loss: 0.4235 (0.4010)\n",
      "Epoch: [13][20/99] Time: 0.446 (0.619) Data: 0.011 (0.187) Loss: 1.5999 (1.6031) Sup Loss: 0.2754 (0.3890)\n",
      "Epoch: [13][30/99] Time: 0.454 (0.559) Data: 0.004 (0.129) Loss: 1.6383 (1.6013) Sup Loss: 0.4626 (0.3755)\n",
      "Epoch: [13][40/99] Time: 0.408 (0.526) Data: 0.007 (0.099) Loss: 1.5929 (1.5964) Sup Loss: 0.3345 (0.3727)\n",
      "Epoch: [13][50/99] Time: 0.415 (0.507) Data: 0.006 (0.081) Loss: 1.5426 (1.5992) Sup Loss: 0.3420 (0.3769)\n",
      "Epoch: [13][60/99] Time: 0.403 (0.495) Data: 0.004 (0.069) Loss: 1.5163 (1.5981) Sup Loss: 0.2672 (0.3821)\n",
      "Epoch: [13][70/99] Time: 0.406 (0.482) Data: 0.003 (0.060) Loss: 1.6797 (1.6005) Sup Loss: 0.5670 (0.3800)\n",
      "Epoch: [13][80/99] Time: 0.393 (0.474) Data: 0.005 (0.053) Loss: 1.5547 (1.5990) Sup Loss: 0.3533 (0.3851)\n",
      "Epoch: [13][90/99] Time: 0.431 (0.467) Data: 0.005 (0.048) Loss: 1.5737 (1.5982) Sup Loss: 0.5117 (0.3846)\n",
      "\n",
      "Epoch: 14\n",
      "Epoch: [14][0/99] Time: 4.367 (4.367) Data: 3.785 (3.785) Loss: 1.6135 (1.6135) Sup Loss: 0.4526 (0.4526)\n",
      "Epoch: [14][10/99] Time: 0.465 (0.813) Data: 0.023 (0.355) Loss: 1.6156 (1.5924) Sup Loss: 0.2848 (0.3739)\n",
      "Epoch: [14][20/99] Time: 0.462 (0.634) Data: 0.028 (0.190) Loss: 1.6362 (1.5768) Sup Loss: 0.3303 (0.3942)\n",
      "Epoch: [14][30/99] Time: 0.416 (0.568) Data: 0.005 (0.131) Loss: 1.6517 (1.5783) Sup Loss: 0.5983 (0.3943)\n",
      "Epoch: [14][40/99] Time: 0.450 (0.531) Data: 0.009 (0.101) Loss: 1.4483 (1.5793) Sup Loss: 0.3826 (0.3886)\n",
      "Epoch: [14][50/99] Time: 0.429 (0.511) Data: 0.007 (0.082) Loss: 1.5006 (1.5784) Sup Loss: 0.2898 (0.3806)\n",
      "Epoch: [14][60/99] Time: 0.432 (0.496) Data: 0.004 (0.070) Loss: 1.5428 (1.5765) Sup Loss: 0.6461 (0.3801)\n",
      "Epoch: [14][70/99] Time: 0.431 (0.484) Data: 0.004 (0.061) Loss: 1.5190 (1.5782) Sup Loss: 0.2559 (0.3754)\n",
      "Epoch: [14][80/99] Time: 0.404 (0.475) Data: 0.003 (0.054) Loss: 1.6054 (1.5736) Sup Loss: 0.2388 (0.3776)\n",
      "Epoch: [14][90/99] Time: 0.405 (0.468) Data: 0.003 (0.048) Loss: 1.5661 (1.5719) Sup Loss: 0.3179 (0.3733)\n",
      "\n",
      "Epoch: 15\n",
      "Epoch: [15][0/99] Time: 4.237 (4.237) Data: 3.731 (3.731) Loss: 1.5515 (1.5515) Sup Loss: 0.3493 (0.3493)\n",
      "Epoch: [15][10/99] Time: 0.429 (0.840) Data: 0.004 (0.348) Loss: 1.5939 (1.5618) Sup Loss: 0.3140 (0.3362)\n",
      "Epoch: [15][20/99] Time: 0.454 (0.650) Data: 0.026 (0.187) Loss: 1.5530 (1.5517) Sup Loss: 0.4087 (0.3555)\n",
      "Epoch: [15][30/99] Time: 0.444 (0.579) Data: 0.007 (0.129) Loss: 1.5389 (1.5587) Sup Loss: 0.3166 (0.3623)\n",
      "Epoch: [15][40/99] Time: 0.434 (0.540) Data: 0.008 (0.099) Loss: 1.5787 (1.5626) Sup Loss: 0.3704 (0.3603)\n",
      "Epoch: [15][50/99] Time: 0.415 (0.520) Data: 0.007 (0.081) Loss: 1.4585 (1.5568) Sup Loss: 0.2979 (0.3565)\n",
      "Epoch: [15][60/99] Time: 0.393 (0.506) Data: 0.004 (0.069) Loss: 1.5131 (1.5547) Sup Loss: 0.3609 (0.3594)\n",
      "Epoch: [15][70/99] Time: 0.410 (0.492) Data: 0.003 (0.060) Loss: 1.5220 (1.5500) Sup Loss: 0.4148 (0.3647)\n",
      "Epoch: [15][80/99] Time: 0.400 (0.482) Data: 0.003 (0.053) Loss: 1.5357 (1.5487) Sup Loss: 0.2748 (0.3654)\n",
      "Epoch: [15][90/99] Time: 0.409 (0.475) Data: 0.004 (0.048) Loss: 1.5325 (1.5497) Sup Loss: 0.4783 (0.3616)\n",
      "\n",
      "Epoch: 16\n",
      "Epoch: [16][0/99] Time: 4.228 (4.228) Data: 3.708 (3.708) Loss: 1.5488 (1.5488) Sup Loss: 0.3661 (0.3661)\n",
      "Epoch: [16][10/99] Time: 0.419 (0.794) Data: 0.005 (0.350) Loss: 1.5555 (1.5600) Sup Loss: 0.4149 (0.4165)\n",
      "Epoch: [16][20/99] Time: 0.413 (0.614) Data: 0.008 (0.186) Loss: 1.5875 (1.5596) Sup Loss: 0.3448 (0.3661)\n",
      "Epoch: [16][30/99] Time: 0.412 (0.555) Data: 0.004 (0.129) Loss: 1.5542 (1.5505) Sup Loss: 0.3120 (0.3464)\n",
      "Epoch: [16][40/99] Time: 0.450 (0.521) Data: 0.008 (0.099) Loss: 1.5705 (1.5527) Sup Loss: 0.2796 (0.3417)\n",
      "Epoch: [16][50/99] Time: 0.422 (0.505) Data: 0.007 (0.081) Loss: 1.4248 (1.5480) Sup Loss: 0.3550 (0.3378)\n",
      "Epoch: [16][60/99] Time: 0.414 (0.492) Data: 0.004 (0.069) Loss: 1.5459 (1.5465) Sup Loss: 0.3546 (0.3498)\n",
      "Epoch: [16][70/99] Time: 0.404 (0.480) Data: 0.004 (0.060) Loss: 1.5596 (1.5440) Sup Loss: 0.3890 (0.3422)\n",
      "Epoch: [16][80/99] Time: 0.434 (0.472) Data: 0.005 (0.053) Loss: 1.5411 (1.5424) Sup Loss: 0.4279 (0.3435)\n",
      "Epoch: [16][90/99] Time: 0.404 (0.465) Data: 0.003 (0.048) Loss: 1.4763 (1.5415) Sup Loss: 0.2890 (0.3414)\n",
      "\n",
      "Epoch: 17\n",
      "Epoch: [17][0/99] Time: 4.323 (4.323) Data: 3.678 (3.678) Loss: 1.4442 (1.4442) Sup Loss: 0.3031 (0.3031)\n",
      "Epoch: [17][10/99] Time: 0.444 (0.812) Data: 0.004 (0.346) Loss: 1.4816 (1.4960) Sup Loss: 0.3328 (0.3139)\n",
      "Epoch: [17][20/99] Time: 0.436 (0.633) Data: 0.008 (0.184) Loss: 1.5468 (1.5121) Sup Loss: 0.4809 (0.3216)\n",
      "Epoch: [17][30/99] Time: 0.406 (0.571) Data: 0.008 (0.128) Loss: 1.5928 (1.5161) Sup Loss: 0.2315 (0.3240)\n",
      "Epoch: [17][40/99] Time: 0.428 (0.535) Data: 0.007 (0.098) Loss: 1.4623 (1.5086) Sup Loss: 0.3166 (0.3351)\n",
      "Epoch: [17][50/99] Time: 0.425 (0.515) Data: 0.007 (0.081) Loss: 1.5253 (1.5114) Sup Loss: 0.2595 (0.3347)\n",
      "Epoch: [17][60/99] Time: 0.388 (0.500) Data: 0.003 (0.069) Loss: 1.5238 (1.5102) Sup Loss: 0.4039 (0.3327)\n",
      "Epoch: [17][70/99] Time: 0.437 (0.489) Data: 0.006 (0.060) Loss: 1.4919 (1.5065) Sup Loss: 0.3028 (0.3418)\n",
      "Epoch: [17][80/99] Time: 0.400 (0.479) Data: 0.004 (0.053) Loss: 1.5054 (1.5057) Sup Loss: 0.3481 (0.3368)\n",
      "Epoch: [17][90/99] Time: 0.415 (0.472) Data: 0.005 (0.047) Loss: 1.4724 (1.5042) Sup Loss: 0.3182 (0.3339)\n",
      "\n",
      "Epoch: 18\n",
      "Epoch: [18][0/99] Time: 4.554 (4.554) Data: 3.885 (3.885) Loss: 1.5402 (1.5402) Sup Loss: 0.3311 (0.3311)\n",
      "Epoch: [18][10/99] Time: 0.449 (0.822) Data: 0.004 (0.363) Loss: 1.4373 (1.5069) Sup Loss: 0.2945 (0.3312)\n",
      "Epoch: [18][20/99] Time: 0.460 (0.641) Data: 0.018 (0.194) Loss: 1.4603 (1.4996) Sup Loss: 0.4307 (0.3459)\n",
      "Epoch: [18][30/99] Time: 0.426 (0.574) Data: 0.004 (0.134) Loss: 1.5816 (1.5029) Sup Loss: 0.3400 (0.3523)\n",
      "Epoch: [18][40/99] Time: 0.443 (0.538) Data: 0.006 (0.103) Loss: 1.5096 (1.5046) Sup Loss: 0.3603 (0.3450)\n",
      "Epoch: [18][50/99] Time: 0.414 (0.515) Data: 0.006 (0.084) Loss: 1.4643 (1.5024) Sup Loss: 0.4268 (0.3415)\n",
      "Epoch: [18][60/99] Time: 0.411 (0.501) Data: 0.004 (0.071) Loss: 1.5111 (1.5017) Sup Loss: 0.2827 (0.3491)\n",
      "Epoch: [18][70/99] Time: 0.435 (0.489) Data: 0.007 (0.062) Loss: 1.4961 (1.4991) Sup Loss: 0.3633 (0.3483)\n",
      "Epoch: [18][80/99] Time: 0.425 (0.480) Data: 0.004 (0.055) Loss: 1.4649 (1.4991) Sup Loss: 0.2101 (0.3400)\n",
      "Epoch: [18][90/99] Time: 0.458 (0.473) Data: 0.006 (0.050) Loss: 1.5022 (1.4985) Sup Loss: 0.2922 (0.3371)\n",
      "\n",
      "Epoch: 19\n",
      "Epoch: [19][0/99] Time: 4.582 (4.582) Data: 3.926 (3.926) Loss: 1.5167 (1.5167) Sup Loss: 0.1969 (0.1969)\n",
      "Epoch: [19][10/99] Time: 0.414 (0.815) Data: 0.007 (0.365) Loss: 1.5211 (1.4859) Sup Loss: 0.2313 (0.2999)\n",
      "Epoch: [19][20/99] Time: 0.435 (0.633) Data: 0.033 (0.195) Loss: 1.4935 (1.4872) Sup Loss: 0.3027 (0.2968)\n",
      "Epoch: [19][30/99] Time: 0.420 (0.568) Data: 0.005 (0.134) Loss: 1.4833 (1.4900) Sup Loss: 0.2606 (0.3203)\n",
      "Epoch: [19][40/99] Time: 0.467 (0.533) Data: 0.015 (0.103) Loss: 1.5189 (1.4875) Sup Loss: 0.3035 (0.3271)\n",
      "Epoch: [19][50/99] Time: 0.399 (0.512) Data: 0.007 (0.084) Loss: 1.5115 (1.4906) Sup Loss: 0.3037 (0.3344)\n",
      "Epoch: [19][60/99] Time: 0.446 (0.498) Data: 0.007 (0.072) Loss: 1.4531 (1.4906) Sup Loss: 0.3944 (0.3321)\n",
      "Epoch: [19][70/99] Time: 0.399 (0.486) Data: 0.005 (0.062) Loss: 1.4525 (1.4853) Sup Loss: 0.4327 (0.3304)\n",
      "Epoch: [19][80/99] Time: 0.414 (0.477) Data: 0.005 (0.055) Loss: 1.4718 (1.4865) Sup Loss: 0.3269 (0.3289)\n",
      "Epoch: [19][90/99] Time: 0.411 (0.471) Data: 0.004 (0.049) Loss: 1.4827 (1.4871) Sup Loss: 0.5289 (0.3340)\n",
      "\n",
      "Epoch: 20\n",
      "Epoch: [20][0/99] Time: 4.754 (4.754) Data: 4.232 (4.232) Loss: 1.4241 (1.4241) Sup Loss: 0.2589 (0.2589)\n",
      "Epoch: [20][10/99] Time: 0.462 (0.831) Data: 0.006 (0.389) Loss: 1.4777 (1.4778) Sup Loss: 0.2474 (0.3159)\n",
      "Epoch: [20][20/99] Time: 0.473 (0.639) Data: 0.044 (0.208) Loss: 1.4717 (1.4865) Sup Loss: 0.3921 (0.3249)\n",
      "Epoch: [20][30/99] Time: 0.482 (0.573) Data: 0.017 (0.143) Loss: 1.4300 (1.4872) Sup Loss: 0.2604 (0.3423)\n",
      "Epoch: [20][40/99] Time: 0.437 (0.538) Data: 0.009 (0.110) Loss: 1.4381 (1.4868) Sup Loss: 0.1787 (0.3448)\n",
      "Epoch: [20][50/99] Time: 0.434 (0.518) Data: 0.007 (0.090) Loss: 1.4779 (1.4830) Sup Loss: 0.2666 (0.3420)\n",
      "Epoch: [20][60/99] Time: 0.409 (0.502) Data: 0.004 (0.076) Loss: 1.4522 (1.4802) Sup Loss: 0.3340 (0.3368)\n",
      "Epoch: [20][70/99] Time: 0.421 (0.489) Data: 0.003 (0.066) Loss: 1.5110 (1.4794) Sup Loss: 0.2951 (0.3381)\n",
      "Epoch: [20][80/99] Time: 0.448 (0.479) Data: 0.008 (0.058) Loss: 1.4183 (1.4754) Sup Loss: 0.1853 (0.3347)\n",
      "Epoch: [20][90/99] Time: 0.441 (0.472) Data: 0.006 (0.053) Loss: 1.4061 (1.4751) Sup Loss: 0.2080 (0.3323)\n",
      "\n",
      "Epoch: 21\n",
      "Epoch: [21][0/99] Time: 4.464 (4.464) Data: 3.964 (3.964) Loss: 1.5282 (1.5282) Sup Loss: 0.4309 (0.4309)\n",
      "Epoch: [21][10/99] Time: 0.439 (0.807) Data: 0.005 (0.367) Loss: 1.4145 (1.4670) Sup Loss: 0.3186 (0.3232)\n",
      "Epoch: [21][20/99] Time: 0.431 (0.628) Data: 0.045 (0.197) Loss: 1.5110 (1.4667) Sup Loss: 0.2894 (0.3137)\n",
      "Epoch: [21][30/99] Time: 0.411 (0.565) Data: 0.004 (0.135) Loss: 1.4420 (1.4620) Sup Loss: 0.2611 (0.3113)\n",
      "Epoch: [21][40/99] Time: 0.468 (0.531) Data: 0.015 (0.104) Loss: 1.4278 (1.4620) Sup Loss: 0.2627 (0.3119)\n"
     ]
    }
   ],
   "source": [
    "print('==> Training Model..')\n",
    "best_acc = 0  # best test accuracy\n",
    "test_log_file = open(args.log_dir + suffix + '.txt', \"w\")\n",
    "test_epoch = 30\n",
    "\n",
    "def trns1(x): # this is more traditionally used \n",
    "    distance = (1-x)/2\n",
    "    return math.exp(-(distance**2)/.01)\n",
    "\n",
    "for epoch in range(300):\n",
    "\n",
    "    # training\n",
    "#     sup_factor = cosine_rampdown(epoch, 130)\n",
    "    sup_factor = sup_factor_schedule(epoch)\n",
    "    train(epoch, unsupervised=False, sup_factor=sup_factor, ema=False)\n",
    "\n",
    "    # testing every 10 epochs\n",
    "    if epoch % test_epoch == 0 or epoch == 299:\n",
    "        net.eval()\n",
    "        print('----------Evaluation---------')\n",
    "        start = time.time()\n",
    "\n",
    "        acc = kNN(epoch, net, trainloader, testloader, 200, args.batch_t, len(trainset), low_dim=args.low_dim)\n",
    "        print(f\"Evaluation Time:{time.time() - start:.2f}\")\n",
    "        writer.add_scalar('Accuracy/nn_acc', acc, epoch)\n",
    "\n",
    "        if acc > best_acc:\n",
    "            print('Saving..')\n",
    "            state = {\n",
    "                'net': net.state_dict(),\n",
    "                'acc': acc,\n",
    "                'epoch': epoch,\n",
    "            }\n",
    "            if not os.path.isdir(args.model_dir):\n",
    "                os.mkdir(args.model_dir)\n",
    "            torch.save(state, args.model_dir + suffix + '.t')\n",
    "            best_acc = acc\n",
    "\n",
    "        print(f'accuracy: {acc:.1f}% \\t (best acc: {best_acc:.1f}%)')\n",
    "        print(f'[Epoch]: {epoch}', file=test_log_file)\n",
    "        print(f'accuracy: {acc:.2f}% \\t (best acc: {best_acc:.2f}%)', file=test_log_file)\n",
    "        \n",
    "        \n",
    "        # Plot the UMAP \n",
    "        feature_mat, label_arr = sem_sup_feature(net, combined_dataloader)\n",
    "        reducer = umap.UMAP(metric = \"cosine\", spread = .1)\n",
    "        feature_and_weight = np.concatenate([feature_mat, net.linear_class.weight.data.cpu().numpy()])\n",
    "        embedding = reducer.fit_transform(feature_and_weight)\n",
    "        \n",
    "        \n",
    "        label_embedding = embedding[label_index]\n",
    "        unlabel_embedding = embedding[unlabel_index]\n",
    "        weight_embedding = embedding[-10:]\n",
    "\n",
    "\n",
    "        fig = plt.figure(figsize=(7,7))\n",
    "        plt.scatter(unlabel_embedding[:,0], unlabel_embedding[:,1], s=1, alpha = 0.8, \n",
    "                    c = label_arr[unlabel_index], cmap=\"tab10\")\n",
    "        plt.grid(True)\n",
    "        plt.tick_params(axis='x', colors=(0,0,0,0))\n",
    "        plt.tick_params(axis='y', colors=(0,0,0,0))\n",
    "        plt.scatter(label_embedding[:,0], label_embedding[:,1], marker='o', color='b', alpha=.4)\n",
    "        plt.scatter(weight_embedding[:,0], weight_embedding[:,1], marker='o', color='r', alpha=.7)\n",
    "        plt.title(\"UMAP -- for trainable weight\")\n",
    "        # plt.savefig('temp')\n",
    "\n",
    "        canvas = FigureCanvas(fig)\n",
    "        ax = fig.gca()\n",
    "\n",
    "        canvas.draw()       # draw the canvas, cache the renderer\n",
    "        width, height = fig.get_size_inches() * fig.get_dpi()\n",
    "        width, height = int(width), int(height)\n",
    "        img = np.fromstring(canvas.tostring_rgb(), dtype='uint8').reshape(height, width, 3)\n",
    "        writer.add_image('UMAP', img, epoch, dataformats = \"HWC\")\n",
    "        \n",
    "        # Do Label Propagation \n",
    "        \n",
    "        Mat_Label = feature_mat[label_index] # Labelled data matrix\n",
    "        labels = label_arr[label_index] # Corresponding labels of Labelled data matrix\n",
    "\n",
    "        Mat_Unlabel = feature_mat[unlabel_index] # UnLabelled data matrix\n",
    "        rest_label  = label_arr[unlabel_index] # Rest of the lable, won't be used\n",
    "        \n",
    "        # Stacking features from labelled examples at the begining\n",
    "        MatX = np.vstack((Mat_Label, Mat_Unlabel))\n",
    "        MatX = MatX / np.linalg.norm(MatX, axis=-1)[:, np.newaxis] # not really required since norm of feature is already 1\n",
    "\n",
    "        affinity_matrix = buildGraph(MatX, trns1, 150) # creating sparse affinity matrix\n",
    "        affinity_matrix_time = time.time()\n",
    "        unlabel_data_labels,_ = labelPropagation(affinity_matrix, Mat_Label, Mat_Unlabel, labels, alpha=.95) # Doing LP \n",
    "\n",
    "        accuracy = get_acc(unlabel_data_labels, rest_label)\n",
    "        \n",
    "        print(f'Label Propagation accuracy is {accuracy:.4f}', file=test_log_file)\n",
    "        test_log_file.flush()\n",
    "        writer.add_scalar('Accuracy/LP_acc', accuracy, epoch)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_mat, label_arr = sem_sup_feature(net, combined_dataloader)\n",
    "Mat_Label = feature_mat[label_index] #Labelled data matrix\n",
    "labels = label_arr[label_index] #Corresponding labels of Labelled data matrix\n",
    "\n",
    "Mat_Unlabel = feature_mat[unlabel_index] #UnLabelled data matrix\n",
    "rest_label  = label_arr[unlabel_index] #Rest of the lable, won't be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.7467 and time for affinity matrix 2 seconds, label propagation time 2 seconds\n"
     ]
    }
   ],
   "source": [
    "# Example of how to do label propagation\n",
    "start_time = time.time()\n",
    "# Two types of disnatce function\n",
    "def trns(x): # taken from this paper https://arxiv.org/pdf/1904.04717.pdf\n",
    "    return 0 if x < 0 else x**9\n",
    "\n",
    "def trns1(x): # this is more traditionally used \n",
    "    distance = (1-x)/2\n",
    "    return math.exp(-(distance**2)/.01)\n",
    "\n",
    "#Stacking features from labelled examples at the begining\n",
    "MatX = np.vstack((Mat_Label, Mat_Unlabel))\n",
    "MatX = MatX / np.linalg.norm(MatX, axis=-1)[:, np.newaxis] # not really required since norm of feature is already 1\n",
    "\n",
    "affinity_matrix = buildGraph(MatX, trns, 20) # creating sparse affinity matrix\n",
    "affinity_matrix_time = time.time()\n",
    "unlabel_data_labels, unlabel_class_prob = labelPropagation(affinity_matrix, Mat_Label, Mat_Unlabel, labels, alpha=.95) # Doing LP \n",
    "\n",
    "accuracy = get_acc(unlabel_data_labels, rest_label) # Measuring accuracy \n",
    "time_taken = time.time() - start_time\n",
    "print(f\"Accuracy is {accuracy:.4f} and time for affinity matrix {affinity_matrix_time - start_time:.0f} seconds, label propagation time {time.time() - affinity_matrix_time:.0f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabel_class_prob = unlabel_class_prob[250:]\n",
    "unlabel_entropy = [entropy(p) for p in unlabel_class_prob]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96295"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, indices = torch.topk(torch.tensor(unlabel_entropy), 20000, largest=False)\n",
    "indices = indices.tolist()\n",
    "get_acc(unlabel_data_labels[indices], rest_label[indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc(predicted_labels, true_labels):\n",
    "    '''returns accuracy'''\n",
    "    \n",
    "    corrects = 0\n",
    "    num_samples = len(predicted_labels)\n",
    "    incorrect_index = []\n",
    "    for i in range(num_samples):\n",
    "        if predicted_labels[i] == true_labels[i]:\n",
    "            corrects +=1\n",
    "        else:\n",
    "            incorrect_index.append(i)\n",
    "    return corrects/num_samples, incorrect_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([68., 67., 89., 69., 88., 58., 70., 79., 80., 73.]),\n",
       " array([0. , 0.9, 1.8, 2.7, 3.6, 4.5, 5.4, 6.3, 7.2, 8.1, 9. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_, inc_ind = get_acc(unlabel_data_labels[indices], rest_label[indices])\n",
    "plt.hist(rest_label[inc_ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "885"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "973-88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1216., 4254.,  957., 1022.,  973.,  856., 1970., 2851., 1960.,\n",
       "        3941.]),\n",
       " array([0. , 0.9, 1.8, 2.7, 3.6, 4.5, 5.4, 6.3, 7.2, 8.1, 9. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAATcklEQVR4nO3db6zc1Z3f8fdnDSVssiggLoj4WjWNvO0CUky5ct0iVWmIihtWa/IAyZE2oArJESItqVJtcZ5s9oElqubPlqogOX+K6WaDrCQrrITsLksTrSKxeC+ExRiCsAKFG7v4bqo0pA+8tfPtgzmIiRnfO9fXnoF73i9pNL/5zjnzOzOyP/75zG9+J1WFJKkPvzbtAUiSJsfQl6SOGPqS1BFDX5I6YuhLUkfOm/YAlnPppZfWxo0bpz0MSXpHefLJJ/+2qmZOrb/tQ3/jxo3Mz89PexiS9I6S5H+Oqju9I0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHXnb/yL3nWjj3d+Z2r5fvuemqe1b0tufR/qS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjowd+knWJflhkm+3x5ckeTTJi+3+4qG2u5IcTvJCkhuH6tclOdieuzdJzu7bkSQtZSVH+ncBzw89vht4rKo2AY+1xyS5CtgBXA1sA+5Lsq71uR/YCWxqt22rGr0kaUXGCv0ks8BNwJeHytuBvW17L3DzUP2hqjpeVS8Bh4EtSa4ALqqqx6uqgAeH+kiSJmDcI/0/BH4P+OVQ7fKqOgrQ7i9r9fXAq0PtFlptfds+tf4WSXYmmU8yv7i4OOYQJUnLWTb0k/w2cKyqnhzzNUfN09cS9bcWq/ZU1VxVzc3MzIy5W0nScsa5yub1wO8k+QjwLuCiJH8EvJbkiqo62qZujrX2C8CGof6zwJFWnx1RlyRNyLKhX1W7gF0AST4I/Puq+t0k/wm4Dbin3T/cuuwH/jjJF4D3MfjC9kBVnUzyepKtwBPArcB/OcvvR5LOqmldKv1cXSZ9NdfTvwfYl+R24BXgFoCqOpRkH/AccAK4s6pOtj53AA8AFwLfbTdJ0oSsKPSr6vvA99v2T4EbTtNuN7B7RH0euGalg5QknR3+IleSOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOjLNG7ruSHEjyN0kOJfmDVv9skp8kebrdPjLUZ1eSw0leSHLjUP26JAfbc/cmGbVuriTpHBlnEZXjwIeq6hdJzgd+kOSNFa++WFWfG26c5CpgB3A1g+US/yLJb7bVs+4HdgJ/BTwCbMPVsyRpYpY90q+BX7SH57dbLdFlO/BQVR2vqpeAw8CWtnj6RVX1eFUV8CBw8+qGL0laibHm9JOsS/I0cAx4tKqeaE99MskzSb6a5OJWWw+8OtR9odXWt+1T66P2tzPJfJL5xcXFFbwdSdJSxgr9qjpZVZuBWQZH7dcwmKp5P7AZOAp8vjUfNU9fS9RH7W9PVc1V1dzMzMw4Q5QkjWFFZ+9U1c8YLIy+rapea/8Y/BL4ErClNVsANgx1mwWOtPrsiLokaULGOXtnJsl72/aFwIeBH7U5+jd8FHi2be8HdiS5IMmVwCbgQFUdBV5PsrWdtXMr8PBZfC+SpGWMc/bOFcDeJOsY/COxr6q+neS/J9nMYIrmZeATAFV1KMk+4DngBHBnO3MH4A7gAeBCBmfteOaOJE3QsqFfVc8A146of3yJPruB3SPq88A1KxyjJOks8Re5ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOjLNc4ruSHEjyN0kOJfmDVr8kyaNJXmz3Fw/12ZXkcJIXktw4VL8uycH23L1t2URJ0oSMc6R/HPhQVX0A2AxsS7IVuBt4rKo2AY+1xyS5CtgBXA1sA+5rSy0C3A/sZLBu7qb2vCRpQpYN/Rr4RXt4frsVsB3Y2+p7gZvb9nbgoao6XlUvAYeBLW0h9Yuq6vGqKuDBoT6SpAkYa04/ybokTwPHgEer6gng8qo6CtDuL2vN1wOvDnVfaLX1bfvU+qj97Uwyn2R+cXFxJe9HkrSEsUK/qk5W1WZglsFR+1KLm4+ap68l6qP2t6eq5qpqbmZmZpwhSpLGsKKzd6rqZ8D3GczFv9ambGj3x1qzBWDDULdZ4Eirz46oS5ImZJyzd2aSvLdtXwh8GPgRsB+4rTW7DXi4be8HdiS5IMmVDL6wPdCmgF5PsrWdtXPrUB9J0gScN0abK4C97QycXwP2VdW3kzwO7EtyO/AKcAtAVR1Ksg94DjgB3FlVJ9tr3QE8AFwIfLfdJEkTsmzoV9UzwLUj6j8FbjhNn93A7hH1eWCp7wMkSeeQv8iVpI6MM70jSQBsvPs7U9nvy/fcNJX9rkUe6UtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHVknJWzNiT5XpLnkxxKclerfzbJT5I83W4fGeqzK8nhJC8kuXGofl2Sg+25e9sKWpKkCRnn0songE9X1VNJfgN4Msmj7bkvVtXnhhsnuQrYAVwNvA/4iyS/2VbPuh/YCfwV8AiDtXZdPUuSJmTZI/2qOlpVT7Xt14HngfVLdNkOPFRVx6vqJeAwsKUtnn5RVT1eVQU8CNy86ncgSRrbiub0k2xksHTiE630ySTPJPlqkotbbT3w6lC3hVZb37ZPrY/az84k80nmFxcXVzJESdISxg79JO8Bvgl8qqp+zmCq5v3AZuAo8Pk3mo7oXkvU31qs2lNVc1U1NzMzM+4QJUnLGCv0k5zPIPC/VlXfAqiq16rqZFX9EvgSsKU1XwA2DHWfBY60+uyIuiRpQsY5eyfAV4Dnq+oLQ/Urhpp9FHi2be8HdiS5IMmVwCbgQFUdBV5PsrW95q3Aw2fpfUiSxjDO2TvXAx8HDiZ5utU+A3wsyWYGUzQvA58AqKpDSfYBzzE48+fOduYOwB3AA8CFDM7a8cwdSZqgZUO/qn7A6Pn4R5bosxvYPaI+D1yzkgFKks4ef5ErSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSR8ZZOWtDku8leT7JoSR3tfolSR5N8mK7v3ioz64kh5O8kOTGofp1SQ625+5tK2hJkiZknCP9E8Cnq+q3gK3AnUmuAu4GHquqTcBj7THtuR3A1cA24L4k69pr3Q/sZLCE4qb2vCRpQpYN/ao6WlVPte3XgeeB9cB2YG9rthe4uW1vBx6qquNV9RJwGNjS1tS9qKoer6oCHhzqI0magBXN6SfZCFwLPAFc3hY7p91f1pqtB14d6rbQauvb9qn1UfvZmWQ+yfzi4uJKhihJWsLYoZ/kPcA3gU9V1c+XajqiVkvU31qs2lNVc1U1NzMzM+4QJUnLWHZhdIAk5zMI/K9V1bda+bUkV1TV0TZ1c6zVF4ANQ91ngSOtPjuiLmmFNt79nWkPYaJ6e7/n0jhn7wT4CvB8VX1h6Kn9wG1t+zbg4aH6jiQXJLmSwRe2B9oU0OtJtrbXvHWojyRpAsY50r8e+DhwMMnTrfYZ4B5gX5LbgVeAWwCq6lCSfcBzDM78ubOqTrZ+dwAPABcC3203SdKELBv6VfUDRs/HA9xwmj67gd0j6vPANSsZoCTp7PEXuZLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0JekjoyzXOJXkxxL8uxQ7bNJfpLk6Xb7yNBzu5IcTvJCkhuH6tclOdieu7ctmShJmqBxjvQfALaNqH+xqja32yMASa4CdgBXtz73JVnX2t8P7GSwZu6m07ymJOkcWjb0q+ovgf895uttBx6qquNV9RJwGNiS5Argoqp6vKoKeBC4+UwHLUk6M6uZ0/9kkmfa9M/FrbYeeHWozUKrrW/bp9ZHSrIzyXyS+cXFxVUMUZI07ExD/37g/cBm4Cjw+VYfNU9fS9RHqqo9VTVXVXMzMzNnOERJ0qnOKPSr6rWqOllVvwS+BGxpTy0AG4aazgJHWn12RF2SNEFnFPptjv4NHwXeOLNnP7AjyQVJrmTwhe2BqjoKvJ5kaztr51bg4VWMW5J0Bs5brkGSrwMfBC5NsgD8PvDBJJsZTNG8DHwCoKoOJdkHPAecAO6sqpPtpe5gcCbQhcB3202SNEHLhn5VfWxE+StLtN8N7B5RnweuWdHoJElnlb/IlaSOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHVk2R9nvZNtvPs70x6CJL2teKQvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOrJs6LeFz48leXaodkmSR5O82O4vHnpuV5LDSV5IcuNQ/bokB9tz97YVtCRJEzTOkf4DwLZTancDj1XVJuCx9pgkVwE7gKtbn/uSrGt97gd2MlhCcdOI15QknWPjrJz1l0k2nlLezmAJRYC9wPeB/9DqD1XVceClJIeBLUleBi6qqscBkjwI3IxLJq4Z0/wh3Mv33DS1fUvvNGc6p395W+ycdn9Zq68HXh1qt9Bq69v2qfWRkuxMMp9kfnFx8QyHKEk61dm+DMOoefpaoj5SVe0B9gDMzc2dtp0E0/tfhv/D0DvRmYb+a0muqKqjSa4AjrX6ArBhqN0scKTVZ0fUdZZ5vSFJSznT6Z39wG1t+zbg4aH6jiQXJLmSwRe2B9oU0OtJtrazdm4d6iNJmpBlj/STfJ3Bl7aXJlkAfh+4B9iX5HbgFeAWgKo6lGQf8BxwArizqk62l7qDwZlAFzL4AtcvcSVpwsY5e+djp3nqhtO03w3sHlGfB65Z0egkSWeVv8iVpI6s6UVUpHPJL831TuSRviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkdWFfpJXk5yMMnTSeZb7ZIkjyZ5sd1fPNR+V5LDSV5IcuNqBy9JWpmzcaT/L6pqc1XNtcd3A49V1SbgsfaYJFcBO4CrgW3AfUnWnYX9S5LGdC6md7YDe9v2XuDmofpDVXW8ql4CDgNbzsH+JUmnsdrQL+DPkzyZZGerXd4WQqfdX9bq64FXh/outNpbJNmZZD7J/OLi4iqHKEl6w2pXzrq+qo4kuQx4NMmPlmibEbUa1bCq9gB7AObm5ka2kSSt3KqO9KvqSLs/BvwJg+ma15JcAdDuj7XmC8CGoe6zwJHV7F+StDJnHPpJ3p3kN97YBv4l8CywH7itNbsNeLht7wd2JLkgyZXAJuDAme5fkrRyq5neuRz4kyRvvM4fV9WfJvlrYF+S24FXgFsAqupQkn3Ac8AJ4M6qOrmq0UuSVuSMQ7+qfgx8YET9p8ANp+mzG9h9pvuUJK2Ov8iVpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHVk4qGfZFuSF5IcTnL3pPcvST2baOgnWQf8V+BfAVcBH0ty1STHIEk9m/SR/hbgcFX9uKr+DngI2D7hMUhSt1azMPqZWA+8OvR4AfgnpzZKshPY2R7+IskLZ7i/S4G/PcO+a5Gfx5v8LH6Vn8eb3hafRf7jql/i748qTjr0M6JWbylU7QH2rHpnyXxVza32ddYKP483+Vn8Kj+PN631z2LS0zsLwIahx7PAkQmPQZK6NenQ/2tgU5Irk/w9YAewf8JjkKRuTXR6p6pOJPkk8GfAOuCrVXXoHO5y1VNEa4yfx5v8LH6Vn8eb1vRnkaq3TKlLktYof5ErSR0x9CWpI2sy9L3Uw5uSbEjyvSTPJzmU5K5pj2nakqxL8sMk3572WKYtyXuTfCPJj9qfkX867TFNU5J/1/6ePJvk60neNe0xnW1rLvS91MNbnAA+XVW/BWwF7uz88wC4C3h+2oN4m/jPwJ9W1T8CPkDHn0uS9cC/Beaq6hoGJ5vsmO6ozr41F/p4qYdfUVVHq+qptv06g7/U66c7qulJMgvcBHx52mOZtiQXAf8c+ApAVf1dVf1suqOauvOAC5OcB/w6a/B3RGsx9Edd6qHbkBuWZCNwLfDEdEcyVX8I/B7wy2kP5G3gHwCLwH9r011fTvLuaQ9qWqrqJ8DngFeAo8D/qao/n+6ozr61GPpjXeqhN0neA3wT+FRV/Xza45mGJL8NHKuqJ6c9lreJ84B/DNxfVdcC/xfo9juwJBczmBW4Engf8O4kvzvdUZ19azH0vdTDKZKczyDwv1ZV35r2eKboeuB3krzMYNrvQ0n+aLpDmqoFYKGq3vif3zcY/CPQqw8DL1XVYlX9P+BbwD+b8pjOurUY+l7qYUiSMJizfb6qvjDt8UxTVe2qqtmq2sjgz8X/qKo1dyQ3rqr6X8CrSf5hK90APDfFIU3bK8DWJL/e/t7cwBr8YnvSV9k856ZwqYe3u+uBjwMHkzzdap+pqkemOCa9ffwb4GvtAOnHwL+e8nimpqqeSPIN4CkGZ739kDV4SQYvwyBJHVmL0zuSpNMw9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JH/j/ghJlDMFOvcQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(rest_label[indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_list = []\n",
    "for l in list(range(750, 50750, 1000)):\n",
    "    _, indices = torch.topk(torch.tensor(unlabel_entropy), l, largest=False)\n",
    "    indices = indices.tolist()\n",
    "    acc_list.append(get_acc(unlabel_data_labels[indices], rest_label[indices]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(list(range(750, 50750, 1000)), acc_list)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_mat, label_arr = sem_sup_feature(net, combined_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import umap\n",
    "# reducer = umap.UMAP(metric = \"cosine\", spread = .1)\n",
    "# feature_and_weight = np.concatenate([feature_mat[unlabel_index], net.linear_class.weight.data.cpu().numpy()])\n",
    "# embedding = reducer.fit_transform(feature_and_weight)\n",
    "\n",
    "# unlabel_embedding = embedding[:len(unlabel_index)]\n",
    "# label_embedding = embedding[[unlabel_index.index(i) for i in label_index]]\n",
    "# weight_embedding = embedding[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                                                                                                                                                                       # fig = plt.figure(figsize=(7,7))\n",
    "# plt.scatter(unlabel_embedding[:,0], unlabel_embedding[:,1], s=1, alpha = 0.8, \n",
    "#             c = label_arr[unlabel_index], cmap=\"tab10\")\n",
    "# plt.grid(True)\n",
    "# plt.tick_params(axis='x', colors=(0,0,0,0))\n",
    "# plt.tick_params(axis='y', colors=(0,0,0,0))\n",
    "# plt.scatter(label_embedding[:,0], label_embedding[:,1], marker='o', color='b', alpha=.4)\n",
    "# plt.scatter(weight_embedding[:,0], weight_embedding[:,1], marker='o', color='r', alpha=.7)\n",
    "# plt.title(\"UMAP -- for trainable weight\")\n",
    "# # plt.savefig('temp')\n",
    "\n",
    "# canvas = FigureCanvas(fig)\n",
    "# ax = fig.gca()\n",
    "\n",
    "# # ax.text(0.0,0.0,\"Test\", fontsize=45)\n",
    "# # ax.axis('off')\n",
    "\n",
    "# canvas.draw()       # draw the canvas, cache the renderer\n",
    "# width, height = fig.get_size_inches() * fig.get_dpi()\n",
    "# width, height = int(width), int(height)\n",
    "# img = np.fromstring(canvas.tostring_rgb(), dtype='uint8').reshape(height, width, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch] *",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}